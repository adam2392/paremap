{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis Of Data\n",
    "Here I just want to have a comprehensive check of my data.\n",
    "\n",
    "1. format of data\n",
    "2. nans/infs?\n",
    "3. range of data\n",
    "4. shapes of data (features, samples)\n",
    "5. labels of data?\n",
    "6. missing data? (not relevant for paremap probably)\n",
    "\n",
    "Input subject, and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial import distance as Distance\n",
    "\n",
    "# pretty charting\n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Format Of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Extract wordpairs data into a dictionary for a subject/session/block\n",
    "#### dictionary{wordpair:{channels}}\n",
    "def extractSubjSessionBlockData(subj, session, block):\n",
    "    # file directory for a subj/session/block\n",
    "    filedir = '../../condensed_data_' + subj + '/blocks/' + session + '/' + block\n",
    "    wordpairs = os.listdir(filedir) \n",
    "    \n",
    "    # initialize data dictionary with meta data\n",
    "    data_dict = {}\n",
    "    data_dict['meta'] = {'subject': subj,\n",
    "                         'session': session,\n",
    "                         'block': block}\n",
    "    \n",
    "    for wordpair in wordpairs:    # loop thru all wordpairs\n",
    "        wordpair_dir = filedir + '/' + wordpair\n",
    "        all_channel_mats = os.listdir(wordpair_dir)\n",
    "        \n",
    "        data_dict[wordpair] = {}\n",
    "        for channel in all_channel_mats: # loop thru all channels\n",
    "            chan_file = wordpair_dir + '/' + channel\n",
    "\n",
    "            ## 00: load in data\n",
    "            data = scipy.io.loadmat(chan_file)\n",
    "            data = data['data']\n",
    "            \n",
    "            ## 01: get the time point for probeword on\n",
    "            timeZero = data['timeZero'][0][0][0]\n",
    "        \n",
    "            ## 02: get the time point of vocalization\n",
    "            vocalization = data['vocalization'][0][0][0]\n",
    "        \n",
    "            ## 03: Get Power Matrix\n",
    "            power_matrix = data['powerMatZ'][0][0]\n",
    "\n",
    "            chan = channel.split('_')[0]\n",
    "            \n",
    "            # convert channel data into a json dict\n",
    "            data_dict[wordpair][chan] = {'timeZero': timeZero,\n",
    "                                          'timeVocalization':vocalization,\n",
    "                                          'powerMat': power_matrix}\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing subject:  NIH034\n",
      "The sessions:  ['session_1', 'session_2']\n",
      "The blocks are: \n",
      "['BLOCK_0', 'BLOCK_1', 'BLOCK_2', 'BLOCK_3', 'BLOCK_4', 'BLOCK_5']  \n",
      "\n",
      "The word pairs for this session/block: \n",
      "['BRICK_CLOCK', 'CLOCK_BRICK', 'GLASS_JUICE', 'JUICE_GLASS']\n",
      "Subject:  NIH034\n",
      "Session:  session_1\n",
      "Block:  BLOCK_0\n",
      "['BRICK_CLOCK', 'GLASS_JUICE', 'meta', 'JUICE_GLASS', 'CLOCK_BRICK']\n",
      "{'session': 'session_1', 'block': 'BLOCK_0', 'subject': 'NIH034'} \n",
      "\n",
      "\n",
      "The type of data input for power matrix is:  <type 'numpy.ndarray'>\n",
      "The shape of each power matrix is (approximately):  (20, 7, 149)\n",
      "The range of data goes from:  -2.82848697831  to  2.29858514626\n"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH034' # change the directories if you want\n",
    "filedir = '../../condensed_data_' + subj + '/blocks/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:] # change which sessions we want\n",
    "print \"Analyzing subject: \", subj\n",
    "print \"The sessions: \", sessions\n",
    "\n",
    "# loop through each session\n",
    "for idx, session in enumerate(sessions):\n",
    "    # the session directory\n",
    "    sessiondir = filedir + sessions[idx]\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "    print \"The blocks are: \\n\", blocks, ' \\n'\n",
    "    \n",
    "    if len(blocks) != 6: # error check on the directories\n",
    "        print blocks\n",
    "        print(\"Error in the # of blocks. There should be 5.\")\n",
    "        break\n",
    "    \n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 1):\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "        print \"The word pairs for this session/block: \\n\", wordpairs\n",
    "        a_wordpair = wordpairs[0]\n",
    "        \n",
    "        print 'Subject: ', subj\n",
    "        print 'Session: ', session\n",
    "        print 'Block: ', block\n",
    "        \n",
    "        block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "        print block_data.keys()\n",
    "        \n",
    "        print block_data['meta'], '\\n\\n'\n",
    "        print \"The type of data input for power matrix is: \", type(block_data['BRICK_CLOCK']['1']['powerMat'])\n",
    "        print \"The shape of each power matrix is (approximately): \", block_data['BRICK_CLOCK']['1']['powerMat'].shape\n",
    "        print \"The range of data goes from: \", np.amin(block_data['BRICK_CLOCK']['1']['powerMat']), \" to \", np.amax(block_data['BRICK_CLOCK']['1']['powerMat'])\n",
    "\n",
    "        \n",
    "        break\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NIH039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractSubjSessionBlockData(subj, session, block):\n",
    "    # file directory for a subj/session/block\n",
    "    filedir = '../../condensed_data_' + subj + '/sessions/' + session + '/' + block\n",
    "    wordpairs = os.listdir(filedir) \n",
    "    \n",
    "    # initialize data dictionary with meta data\n",
    "    data_dict = {}\n",
    "    data_dict['meta'] = {'subject': subj,\n",
    "                         'session': session,\n",
    "                         'block': block}\n",
    "    data_dict['data'] = {}\n",
    "    for wordpair in wordpairs:    # loop thru all wordpairs\n",
    "        wordpair_dir = filedir + '/' + wordpair\n",
    "        all_channel_mats = os.listdir(wordpair_dir)\n",
    "        \n",
    "        data_dict['data'][wordpair] = {}\n",
    "        for channel in all_channel_mats: # loop thru all channels\n",
    "            chan_file = wordpair_dir + '/' + channel\n",
    "\n",
    "            ## 00: load in data\n",
    "            data = scipy.io.loadmat(chan_file)\n",
    "            data = data['data']\n",
    "            \n",
    "            ## 01: get the time point for probeword on\n",
    "            timeZero = data['timeZero'][0][0][0]\n",
    "        \n",
    "            ## 02: get the time point of vocalization\n",
    "            vocalization = data['vocalization'][0][0][0]\n",
    "        \n",
    "            ## 03: Get Power Matrix\n",
    "            power_matrix = data['powerMatZ'][0][0]\n",
    "            \n",
    "            ## 04: Get absolute response times for plotting\n",
    "            responseTimes = data['originalResponseTimes'][0][0][0]\n",
    "            \n",
    "            ## 05: Get probe word and target\n",
    "            probeWord = data['probeWord'][0][0][0]\n",
    "            targetWord = data['targetWord'][0][0][0]\n",
    "            \n",
    "            chan = channel.split('_')[0]\n",
    "            \n",
    "            # convert channel data into a json dict\n",
    "            data_dict['data'][wordpair][chan] = {'timeZero': timeZero,\n",
    "                                          'timeVocalization':vocalization,\n",
    "                                          'powerMat': power_matrix,\n",
    "                                          'responseTimes': responseTimes,\n",
    "                                          'probeWord': probeWord,\n",
    "                                          'targetWord': targetWord}\n",
    "    \n",
    "    data_dict['meta']['description'] = data['description'][0][0][0]\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing subject:  NIH039\n",
      "The sessions:  ['session_3']\n",
      "The blocks are: \n",
      "['BLOCK_0', 'BLOCK_1', 'BLOCK_2', 'BLOCK_3', 'BLOCK_4', 'BLOCK_5']  \n",
      "\n",
      "The word pairs for this session/block: \n",
      "['BRICK_CLOCK', 'CLOCK_BRICK', 'GLASS_JUICE', 'JUICE_GLASS']\n",
      "Subject:  NIH039\n",
      "Session:  session_3\n",
      "Block:  BLOCK_0\n",
      "['meta', 'data']\n",
      "{'session': 'session_3', 'description': u'100 ms windows, 50 ms overlap. With eeg data from -1 to 5 seconds after probeWordOn.', 'block': 'BLOCK_0', 'subject': 'NIH039'} \n",
      "\n",
      "\n",
      "The type of data input for power matrix is:  <type 'numpy.ndarray'>\n",
      "The shape of each power matrix is (approximately):  (20, 7, 119)\n",
      "The range of data goes from:  -2.66495306147  to  3.05841664274\n",
      "The number of channels are:  72\n"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH039' # change the directories if you want\n",
    "filedir = '../../condensed_data_' + subj + '/sessions/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:] # change which sessions we want\n",
    "print \"Analyzing subject: \", subj\n",
    "print \"The sessions: \", sessions\n",
    "\n",
    "# loop through each session\n",
    "for idx, session in enumerate(sessions):\n",
    "    # the session directory\n",
    "    sessiondir = filedir + sessions[idx]\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "    print \"The blocks are: \\n\", blocks, ' \\n'\n",
    "    \n",
    "    if len(blocks) != 6: # error check on the directories\n",
    "        print blocks\n",
    "        print(\"Error in the # of blocks. There should be 5.\")\n",
    "        break\n",
    "    \n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 1):\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "        print \"The word pairs for this session/block: \\n\", wordpairs\n",
    "        a_wordpair = wordpairs[0]\n",
    "        \n",
    "        print 'Subject: ', subj\n",
    "        print 'Session: ', session\n",
    "        print 'Block: ', block\n",
    "        \n",
    "        block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "        print block_data.keys()\n",
    "        \n",
    "        print block_data['meta'], '\\n\\n'\n",
    "        print \"The type of data input for power matrix is: \", type(block_data['data']['BRICK_CLOCK']['1']['powerMat'])\n",
    "        print \"The shape of each power matrix is (approximately): \", block_data['data']['BRICK_CLOCK']['1']['powerMat'].shape\n",
    "        print \"The range of data goes from: \", np.amin(block_data['data']['BRICK_CLOCK']['1']['powerMat']), \" to \", np.amax(block_data['data']['BRICK_CLOCK']['1']['powerMat'])\n",
    "        print \"The number of channels are: \", len(block_data['data']['BRICK_CLOCK'].keys())\n",
    "        \n",
    "        break\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  3.,  5.,  2.,  1.,  2.,  0.,  0.,  1.,  1.]),\n",
       " array([ 0.66188672,  0.7634679 ,  0.86504907,  0.96663025,  1.06821143,\n",
       "         1.1697926 ,  1.27137378,  1.37295496,  1.47453613,  1.57611731,\n",
       "         1.67769849]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAECCAYAAADn84z1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10633f590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responseTimes =  block_data['data']['BRICK_CLOCK']['1']['responseTimes']/1000.\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(responseTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Check if All The Word Groups are Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createWordGroups(wordpairs):\n",
    "    # create same group pairs\n",
    "    for idx, pair in enumerate(wordpairs):\n",
    "        same_word_group.append([pair, pair])\n",
    "\n",
    "    # create reverse, and different groups\n",
    "    for idx, pairs in enumerate(itertools.combinations(wordpairs,2)):\n",
    "        if isReverse(pairs[0], pairs[1]):\n",
    "            reverse_word_group.append([pairs[0], pairs[1]])\n",
    "        else:\n",
    "            diff_word_group.append([pairs[0], pairs[1]])\n",
    "            \n",
    "    return same_word_group, reverse_word_group, diff_word_group\n",
    "\n",
    "def isReverse(pair1, pair2):\n",
    "    pair1split = pair1.split('_')\n",
    "    pair2split = pair2.split('_')\n",
    "    if pair1split[0] == pair2split[1] and pair1split[1] == pair2split[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing session  session_0\n",
      "BRICK_CLOCK   BRICK _ CLOCK\n",
      "CLOCK_BRICK   CLOCK _ BRICK\n",
      "GLASS_JUICE   GLASS _ JUICE\n",
      "JUICE_GLASS   JUICE _ GLASS\n",
      "Analyzing session  session_1\n",
      "BRICK_CLOCK   BRICK _ CLOCK\n",
      "CLOCK_BRICK   CLOCK _ BRICK\n",
      "GLASS_JUICE   GLASS _ JUICE\n",
      "JUICE_GLASS   JUICE _ GLASS\n",
      "Analyzing session  session_3\n",
      "BRICK_CLOCK   BRICK _ CLOCK\n",
      "CLOCK_BRICK   CLOCK _ BRICK\n",
      "GLASS_JUICE   GLASS _ JUICE\n",
      "JUICE_GLASS   JUICE _ GLASS\n"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH039'\n",
    "filedir = '../../condensed_data_'+ subj +'/sessions/'\n",
    "sessions = os.listdir(filedir)\n",
    "# sessions = sessions[2:]\n",
    "\n",
    "session_pval_dict = {}\n",
    "\n",
    "debug_on = 1\n",
    "# loop through each session\n",
    "for session in sessions:\n",
    "    print \"Analyzing session \", session\n",
    "    sessiondir = filedir + session\n",
    "    \n",
    "    session_pval_diff_mat = np.array(())\n",
    "    session_pval_same_mat = np.array(())\n",
    "    session_pval_reverse_mat = np.array(())\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "    \n",
    "    if len(blocks) != 6: # error check on the directories\n",
    "        print blocks\n",
    "        print(\"Error in the # of blocks. There should be 5.\")\n",
    "        break\n",
    "     \n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 6):\n",
    "        # var for block and directory\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "       \n",
    "        # within-groups analysis only has: SAME, REVERSE, DIFFERENT\n",
    "        diff_word_group = []\n",
    "        reverse_word_group = []\n",
    "        same_word_group = []\n",
    "        \n",
    "        ## 01: Create WordPair Groups\n",
    "        same_word_group, reverse_word_group, diff_word_group = createWordGroups(wordpairs)\n",
    "        \n",
    "        # extract sessionblockdata dictionary\n",
    "        block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "        \n",
    "        chan = 1\n",
    "        ################# 02a: Word Group Check #################\n",
    "        for same_words in same_word_group:\n",
    "            # extract data to process - average across time \n",
    "            same_word_key = same_words[0]\n",
    "            probeWord = block_data['data'][same_word_key][str(chan)]['probeWord']\n",
    "            targetWord = block_data['data'][same_word_key][str(chan)]['targetWord']\n",
    "            \n",
    "            print same_word_key, \" \", probeWord, \"_\", targetWord\n",
    "            \n",
    "        break\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
