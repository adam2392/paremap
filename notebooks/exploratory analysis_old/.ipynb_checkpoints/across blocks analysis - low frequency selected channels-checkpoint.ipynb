{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial import distance as Distance\n",
    "\n",
    "# pretty charting\n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks are: \n",
      "['BLOCK_0', 'BLOCK_1', 'BLOCK_2', 'BLOCK_3', 'BLOCK_4', 'BLOCK_5']\n"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "filedir = '../condensed_data/blocks/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:]\n",
    "print \"Blocks are: \\n\", os.listdir(filedir+sessions[0])\n",
    "\n",
    "def find_same(wordpair, groups):\n",
    "    # split wordpair and reverse\n",
    "    wordsplit = wordpair.split('_')    \n",
    "    try:\n",
    "        sameword_index = groups.index(wordpair)\n",
    "    except:\n",
    "        sameword_index = -1\n",
    "    return sameword_index\n",
    "\n",
    "# functions for finding the different groups of word pairings\n",
    "def find_reverse(wordpair, groups):\n",
    "    # split wordpair and reverse\n",
    "    wordsplit = wordpair.split('_')\n",
    "    wordsplit.reverse()\n",
    "    reverseword = '_'.join(wordsplit)\n",
    "    # find index of reversed word index\n",
    "    try:\n",
    "        reverseword_index = groups.index(reverseword)\n",
    "    except:\n",
    "        reverseword_index = -1\n",
    "    return reverseword_index\n",
    "def find_different(wordpair, groups):\n",
    "    # split wordpair and reverse\n",
    "    wordsplit = wordpair.split('_')    \n",
    "    differentword_index = []\n",
    "    \n",
    "    for idx, group in enumerate(groups):\n",
    "        groupsplit = group.split('_')\n",
    "        if not any(x in groupsplit for x in wordsplit):\n",
    "            differentword_index.append(idx)\n",
    "    \n",
    "    # convert to single number if a list\n",
    "    if len(differentword_index) == 1:\n",
    "        differentword_index = differentword_index[0]\n",
    "    return differentword_index\n",
    "\n",
    "def find_probe(wordpair, groups):\n",
    "    # split wordpair and reverse\n",
    "    wordsplit = wordpair.split('_')    \n",
    "    probeword_index = []\n",
    "    # loop through group of words to check word pair in\n",
    "    for idx, group in enumerate(groups):\n",
    "        groupsplit = group.split('_')\n",
    "        # check if probe word overlaps\n",
    "        if wordsplit[0] == groupsplit[0] and wordsplit[1] != groupsplit[1]:\n",
    "            probeword_index.append(idx)\n",
    "    # convert to single number if a list\n",
    "    if len(probeword_index) != 1 and probeword_index:\n",
    "        print probeword_index\n",
    "        print \"problem in find probe\"\n",
    "    elif not probeword_index: # if no probe words overlap\n",
    "        probeword_index = -1\n",
    "    else:\n",
    "        probeword_index = probeword_index[0]\n",
    "    return probeword_index\n",
    "\n",
    "def find_target(wordpair, groups):\n",
    "    # split wordpair and reverse\n",
    "    wordsplit = wordpair.split('_')    \n",
    "    targetword_index = []\n",
    "    # loop through group of words to check word pair in\n",
    "    for idx, group in enumerate(groups):\n",
    "        groupsplit = group.split('_')\n",
    "        # check if target word overlaps\n",
    "        if wordsplit[1] == groupsplit[1] and wordsplit[0] != groupsplit[0]:\n",
    "            targetword_index.append(idx)\n",
    "    # convert to single number if a list\n",
    "    if len(targetword_index) != 1 and targetword_index:\n",
    "        print targetword_index\n",
    "        print \"problem in find target\"\n",
    "    elif not targetword_index: # if no target words overlap\n",
    "        targetword_index = -1\n",
    "    else:\n",
    "        targetword_index = targetword_index[0]\n",
    "    return targetword_index\n",
    "\n",
    "# check if group is in the list of names\n",
    "def inGroup(group, names):\n",
    "    for i in range(0, len(group)):\n",
    "        if cmpT(group[i],names):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def cmpT(t1, t2): \n",
    "    return sorted(t1) == sorted(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "filedir = '../condensed_data/blocks/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:]\n",
    "\n",
    "session_pval_dict = {}\n",
    "# loop through each session\n",
    "sessiondir = filedir + sessions[0]  \n",
    "# get all blocks for this session\n",
    "blocks = os.listdir(sessiondir)\n",
    "block = blocks[0]\n",
    "block_dir = sessiondir + '/' + block\n",
    "\n",
    "# in each block, get list of word pairs from first and second block\n",
    "wordpairs = os.listdir(block_dir)\n",
    "channels = os.listdir(block_dir+'/'+wordpairs[0])\n",
    "\n",
    "chan_order = []\n",
    "for jdx, chan in sorted(enumerate(channels)):\n",
    "    chan_order.append(chan)\n",
    "# print chan_order\n",
    "chan_order = np.array(chan_order)\n",
    "print len(chan_order)\n",
    "\n",
    "def binarize_pval_mat(pval_mat):\n",
    "    pval_mat[pval_mat > 0.05] = 0.5\n",
    "    pval_mat[pval_mat <= 0.05] = 1\n",
    "    pval_mat[pval_mat == 0.5] = 0\n",
    "    return pval_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Functions to help extract features and plot histogram of distances\n",
    "# loops through each wordpairing group and extract features\n",
    "def extractFeatures(wordgroup, session, blockone, blocktwo):\n",
    "    fig = plt.figure(figsize=(7.5*len(wordgroup), 3))\n",
    "    \n",
    "    for idx, pair in enumerate(wordgroup):\n",
    "        # load in data\n",
    "        first_wordpair_dir = firstblock_dir + '/' + pair[0]\n",
    "        second_wordpair_dir = secondblock_dir + '/' + pair[1]\n",
    "\n",
    "        # initialize np arrays for holding feature vectors for each event\n",
    "        first_pair_features = []\n",
    "        second_pair_features = []\n",
    "\n",
    "        # load in channels\n",
    "        first_channels = os.listdir(first_wordpair_dir)\n",
    "        second_channels = os.listdir(second_wordpair_dir)\n",
    "        # loop through channels\n",
    "        for jdx, chans in enumerate(first_channels):\n",
    "            first_chan_file = first_wordpair_dir + '/' + chans\n",
    "            second_chan_file = second_wordpair_dir + '/' + chans\n",
    "\n",
    "            # load in data\n",
    "            data_first = scipy.io.loadmat(first_chan_file)\n",
    "            data_first = data_first['data']\n",
    "            data_second = scipy.io.loadmat(second_chan_file)\n",
    "            data_second = data_second['data']\n",
    "\n",
    "            ## 06: get the time point for probeword on\n",
    "            first_timeZero = data_first['timeZero'][0][0][0]\n",
    "            second_timeZero = data_second['timeZero'][0][0][0]\n",
    "\n",
    "            ## 07: get the time point of vocalization\n",
    "            first_vocalization = data_first['vocalization'][0][0][0]\n",
    "            second_vocalization = data_second['vocalization'][0][0][0]\n",
    "\n",
    "            ## Power Matrix\n",
    "            first_matrix = data_first['powerMatZ'][0][0]\n",
    "            second_matrix = data_second['powerMatZ'][0][0]\n",
    "            first_matrix = first_matrix[:, freq_bands,:]\n",
    "            second_matrix = second_matrix[:, freq_bands,:]\n",
    "\n",
    "            ### 02: get only the time point before vocalization\n",
    "            first_mean = []\n",
    "            second_mean = []\n",
    "            for i in range(0, len(first_vocalization)):\n",
    "                # either go from timezero -> vocalization, or some other timewindow\n",
    "                first_mean.append(np.mean(first_matrix[i,:,first_timeZero:first_vocalization[i]], axis=1))\n",
    "#                     first_mean.append(np.ndarray.flatten(first_matrix[i,:,first_vocalization[i]-num_time_windows:first_vocalization[i]]))\n",
    "            for i in range(0, len(second_vocalization)):\n",
    "                second_mean.append(np.mean(second_matrix[i,:,second_timeZero:second_vocalization[i]], axis=1))\n",
    "#                     second_mean.append(np.ndarray.flatten(second_matrix[i,:,second_vocalization[i]-num_time_windows:second_vocalization[i]]))\n",
    "\n",
    "            # create feature vector for each event\n",
    "            if jdx == 0:\n",
    "                first_pair_features.append(first_mean)\n",
    "                second_pair_features.append(second_mean)\n",
    "                first_pair_features = np.squeeze(np.array(first_pair_features))\n",
    "                second_pair_features = np.squeeze(np.array(second_pair_features))\n",
    "            else:\n",
    "                first_pair_features = np.concatenate((first_pair_features, first_mean), axis=1)\n",
    "                second_pair_features = np.concatenate((second_pair_features, second_mean), axis=1)\n",
    "        # end of loop through channels\n",
    "        \n",
    "        # compute paired distances between each feature matrix\n",
    "        first_hist, second_hist = computePairDistances(first_pair_features, second_pair_features)\n",
    "        \n",
    "        ## Plotting Paired Distances\n",
    "        plt.subplot(1, len(wordgroup), idx+1)\n",
    "#         plt.subplot(1, 5, )\n",
    "        plt.hist(first_hist, label=pair[0],  lw=3, alpha = 0.75)\n",
    "        plt.hist(second_hist, label=pair[1],  lw=3, alpha = 0.75)\n",
    "        plt.ylim([0.0, 6.0])\n",
    "        plt.xlim([0.0, 1.6])\n",
    "        plt.legend()\n",
    "        plt.title(session + ' comparing ' + pair[0] + '(' + str(len(first_pair_features)) +') vs '+ pair[1] + '(' + str(len(second_pair_features)) +')')\n",
    "\n",
    "# Compute all pairwise distances between first_mat to second_mat\n",
    "def computePairDistances(first_mat, second_mat):\n",
    "    distance_list = []\n",
    "    for idx in range(0, first_mat.shape[0]):\n",
    "        distance_list.append([distances(x, first_mat[idx,:]) for x in second_mat])\n",
    "    distance_list = np.ndarray.flatten(np.array(distance_list))\n",
    "    return distance_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low bands:  ['delta', 'theta', 'alpha']\n",
      "high bands:  ['beta', 'low gamma', 'high gamma', 'HFO']\n",
      "The length of the feature vector for each channel will be:  35  total= 3360\n"
     ]
    }
   ],
   "source": [
    "################################### HYPER-PARAMETERS TO TUNE #######################################################\n",
    "np.random.seed(123456789)  # for reproducibility, set random seed\n",
    "\n",
    "anova_threshold = 90   # how many channels we want to keep\n",
    "distances = Distance.cosine # define distance metric to use\n",
    "num_time_windows = 5\n",
    "low_freq_bands = [0, 1, 2]\n",
    "high_freq_bands = [3, 4, 5, 6]\n",
    "freq_bands = np.arange(0,7,1)\n",
    "\n",
    "freq_labels = ['delta', 'theta', 'alpha', 'beta', 'low gamma', 'high gamma', 'HFO']\n",
    "\n",
    "print 'low bands: ', [freq_labels[i] for i in low_freq_bands]\n",
    "print 'high bands: ', [freq_labels[i] for i in high_freq_bands]\n",
    "print \"The length of the feature vector for each channel will be: \", \\\n",
    "            num_time_windows*len(freq_bands), \\\n",
    "            ' total=', 96*num_time_windows*len(freq_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Analysis Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "filedir = '../condensed_data/blocks/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:]\n",
    "\n",
    "# loop through each session\n",
    "for session in sessions:\n",
    "    print \"Analyzing session \", session\n",
    "    sessiondir = filedir + session\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "    \n",
    "    if len(blocks) != 6: # error check on the directories\n",
    "        print blocks\n",
    "        print(\"Error in the # of blocks. There should be 5.\")\n",
    "        break\n",
    "    \n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 5):\n",
    "        print \"Analyzing block \", blocks[i], ' and ', blocks[i+1]\n",
    "        firstblock = blocks[i]\n",
    "        secondblock = blocks[i+1]\n",
    "        \n",
    "        firstblock_dir = sessiondir+'/'+firstblock\n",
    "        secondblock_dir = sessiondir+'/'+secondblock\n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        first_wordpairs = os.listdir(sessiondir+'/'+firstblock)\n",
    "        second_wordpairs = os.listdir(sessiondir+'/'+secondblock)\n",
    "        \n",
    "        diff_word_group = []\n",
    "        reverse_word_group = []\n",
    "        probe_word_group = []\n",
    "        target_word_group = []\n",
    "        same_word_group = []\n",
    "        \n",
    "        print first_wordpairs\n",
    "        print second_wordpairs\n",
    "        \n",
    "        #### plot meta information about which session and blocks we're analyzing\n",
    "        fig=plt.figure()\n",
    "        axes = plt.gca()\n",
    "        ymin, ymax = axes.get_ylim()\n",
    "        xmin, xmax = axes.get_xlim()\n",
    "        plt.text((xmax-xmin)/4.5, (ymax-ymin)/2, r'Session %s %scomparing %s vs. %s'%(session, '\\n',firstblock, secondblock), fontsize=20)\n",
    "        plt.title(session + ' comparing blocks: ' + firstblock + ' vs. ' + secondblock)\n",
    "        plt.grid(False)\n",
    "        \n",
    "        # go through first block and assign pairs to different groups\n",
    "        for idx, pair in enumerate(first_wordpairs):\n",
    "#             print \"Analyzing \", pair\n",
    "            # obtain indices of: sameword, reverseword, differentwords, probeoverlap, targetoverlap\n",
    "            same_word_index = find_same(pair, second_wordpairs)\n",
    "            reverse_word_index = find_reverse(pair, second_wordpairs)\n",
    "            diff_word_index = find_different(pair, second_wordpairs)\n",
    "            probe_word_index = find_probe(pair, second_wordpairs)\n",
    "            target_word_index = find_target(pair, second_wordpairs)\n",
    "            \n",
    "            # append to list groupings holding pairs of these word groupings\n",
    "            if same_word_index != -1 and not inGroup(same_word_group, [pair, second_wordpairs[same_word_index]]):\n",
    "                same_word_group.append([pair, second_wordpairs[same_word_index]])\n",
    "            if reverse_word_index != -1 and not inGroup(reverse_word_group, [pair, second_wordpairs[reverse_word_index]]): \n",
    "                reverse_word_group.append([pair, second_wordpairs[reverse_word_index]])\n",
    "            if diff_word_index != -1:\n",
    "                if isinstance(diff_word_index, list): # if list, break it down and one pairing at a time\n",
    "                    for diffI in diff_word_index:     # loop through each different word index\n",
    "                        if not inGroup(diff_word_group, [pair, second_wordpairs[diffI]]):\n",
    "                            diff_word_group.append([pair, second_wordpairs[diffI]])\n",
    "                else:\n",
    "                    diff_word_group.append([pair, second_wordpairs[diff_word_index]])\n",
    "            if probe_word_index != -1 and not inGroup(probe_word_group, [pair, second_wordpairs[probe_word_index]]): \n",
    "                probe_word_group.append([pair, second_wordpairs[probe_word_index]])\n",
    "            if target_word_index != -1 and not inGroup(target_word_group, [pair, second_wordpairs[target_word_index]]):\n",
    "                target_word_group.append([pair, second_wordpairs[target_word_index]])\n",
    "        # end of loop through word pairs\n",
    "    # end of loop through block\n",
    "        print same_word_group\n",
    "        print reverse_word_group\n",
    "        print probe_word_group\n",
    "        print target_word_group\n",
    "        print diff_word_group[0:2]\n",
    "        \n",
    "        #### Go through each group and extract the feature data per event\n",
    "        ## 01: Same Word Group\n",
    "        fig = plt.figure(figsize=(15, 3))\n",
    "        extractFeatures(same_word_group, session, firstblock, secondblock)\n",
    "        extractFeatures(reverse_word_group, session, firstblock, secondblock)\n",
    "        extractFeatures(probe_word_group, session, firstblock, secondblock)\n",
    "        extractFeatures(target_word_group, session, firstblock, secondblock)\n",
    "        extractFeatures(diff_word_group[0:2], session, firstblock, secondblock)\n",
    "        \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
