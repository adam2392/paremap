{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification For Channel/Frequency Selection\n",
    "\n",
    "With the paRemap dataset, there are too many frequency bands and channels to analyze. In order to reduce this dimensionality, it would perhaps help to take a systematic approach to selecting relevant channels and frequency bands.\n",
    "\n",
    "Using various classification techniques, perform feature selection on the groups of word pairs.\n",
    "\n",
    "For same_word_group = 0, diff_word_group = 1 labels.\n",
    "\n",
    "1. Every word pair gets spectral matrix subtracted from the first matrix -> get feature vector of freq bands with 5 time points starting from probeWordOn, or before timeVocalization\n",
    "    - only get the number of events that is minimum (e.g. one wordpair has 19 events, another has 18), so there will be 18 events for this group\n",
    "2. For every event, there will be a 7 feature vectors of time points in each frequency band.\n",
    "3. Do this for all blocks, and then for all sessions (e.g. 2 for NIH034)\n",
    "4. Perform classification training and testing on 60/40% of the data to get baseline accuracy\n",
    "5. Repeat by removing 1 of the feature vectors and document accuracy\n",
    "6. Repeat this for all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import os, csv, json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import matplotlib\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy.io\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# pretty charting\n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345678)  # for reproducibility, set random seed\n",
    "\n",
    "names = [\"Linear SVM\", \n",
    "         \"Random Forest\",\n",
    "         \"Quadratic Discriminant Analysis\",\n",
    "        \"Logistic Regression\"]\n",
    "\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Extract wordpairs data into a dictionary for a subject/session/block\n",
    "#### dictionary{wordpair:{channels}}\n",
    "def extractSubjSessionBlockData(subj, session, block):\n",
    "    # file directory for a subj/session/block\n",
    "    filedir = '../../condensed_data_' + subj + '/sessions/' + session + '/' + block\n",
    "    wordpairs = os.listdir(filedir) \n",
    "    \n",
    "    # initialize data dictionary with meta data\n",
    "    data_dict = {}\n",
    "    data_dict['meta'] = {'subject': subj,\n",
    "                         'session': session,\n",
    "                         'block': block}\n",
    "    data_dict['data'] = {}\n",
    "    for wordpair in wordpairs:    # loop thru all wordpairs\n",
    "        wordpair_dir = filedir + '/' + wordpair\n",
    "        all_channel_mats = os.listdir(wordpair_dir)\n",
    "        \n",
    "        data_dict['data'][wordpair] = {}\n",
    "        for channel in all_channel_mats: # loop thru all channels\n",
    "            chan_file = wordpair_dir + '/' + channel\n",
    "\n",
    "            ## 00: load in data\n",
    "            data = scipy.io.loadmat(chan_file)\n",
    "            data = data['data']\n",
    "            \n",
    "            ## 01: get the time point for probeword on\n",
    "            timeZero = data['timeZero'][0][0][0]\n",
    "        \n",
    "            ## 02: get the time point of vocalization\n",
    "            vocalization = data['vocalization'][0][0][0]\n",
    "        \n",
    "            ## 03: Get Power Matrix\n",
    "            power_matrix = data['powerMatZ'][0][0]\n",
    "#             lowbands = np.mean(powerMat[:,0:2,:], axis=1, keepdims=True)\n",
    "#             medbands = powerMat[:,2:4,:]\n",
    "#             highbands = np.mean(powerMat[:,4:,:], axis=1, keepdims=True)\n",
    "#             power_matrix = np.concatenate((lowbands, medbands, highbands), axis=1)\n",
    "            \n",
    "            if np.isnan(power_matrix.any()):\n",
    "                print channel, \" wtf.\"\n",
    "            \n",
    "            chan = channel.split('_')[0]\n",
    "            \n",
    "            # convert channel data into a json dict\n",
    "            data_dict['data'][wordpair][chan] = {'timeZero': timeZero,\n",
    "                                          'timeVocalization':vocalization,\n",
    "                                          'powerMat': power_matrix}\n",
    "    \n",
    "    data_dict['meta']['description'] = data['description'][0][0][0]\n",
    "    \n",
    "#     print \"The size of power matrices are: \", power_matrix.shape\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "def isReverse(pair1, pair2):\n",
    "    pair1split = pair1.split('_')\n",
    "    pair2split = pair2.split('_')\n",
    "    if pair1split[0] == pair2split[1] and pair1split[1] == pair2split[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing subject:  NIH039\n",
      "The sessions are:  ['session_0', 'session_1', 'session_3']\n",
      "(72, 2)\n",
      "Each session has: \n",
      "['BLOCK_0', 'BLOCK_1', 'BLOCK_2', 'BLOCK_3', 'BLOCK_4', 'BLOCK_5']  \n",
      "\n",
      "(673, 4, 6)\n",
      "(1288, 4, 6)\n",
      "(1961, 24)\n",
      "(1961,)\n",
      "Linear SVM\n",
      "Channel:  1\n",
      "[ 0.65680775  0.47477503]\n",
      "Random Forest\n",
      "Channel:  1\n",
      "[ 0.65629781  0.47494315]\n",
      "Quadratic Discriminant Analysis\n",
      "Channel:  1\n",
      "[ 0.57113717  0.49491363]\n",
      "Logistic Regression\n",
      "Channel:  1\n",
      "[ 0.65578786  0.47511066]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "/Users/adam2392/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:102: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "/Users/adam2392/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:125: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "/Users/adam2392/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:127: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH039' # change the directories if you want\n",
    "filedir = '../../condensed_data_' + subj + '/sessions/'\n",
    "sessions = os.listdir(filedir)\n",
    "# sessions = sessions[2:] # change which sessions we want\n",
    "print \"Analyzing subject: \", subj\n",
    "print \"The sessions are: \", sessions\n",
    "\n",
    "baseaccuracy=np.zeros((72,2))\n",
    "print baseaccuracy.shape\n",
    "debug_on = 0\n",
    "\n",
    "channels = np.arange(1, 73, 1)\n",
    "\n",
    "for cdx, channel in enumerate(channels):\n",
    "    channel = str(channel)\n",
    "    \n",
    "    # initialize arrays\n",
    "    same_word_data = np.array(()) # array to store all the feature freq. vectors for a specific word\n",
    "    diff_word_data = np.array(())\n",
    "    \n",
    "    # loop through each session\n",
    "    for idx, session in enumerate(sessions):\n",
    "        # the session directory\n",
    "        sessiondir = filedir + sessions[idx]\n",
    "\n",
    "        # get all blocks for this session\n",
    "        blocks = os.listdir(sessiondir)\n",
    "        if idx==0:\n",
    "            print \"Each session has: \\n\", blocks, ' \\n'\n",
    "\n",
    "        if len(blocks) != 6: # error check on the directories\n",
    "            print blocks\n",
    "            print(\"Error in the # of blocks. There should be 5.\")\n",
    "            break\n",
    "\n",
    "        # loop through each block one at a time, analyze\n",
    "        for i in range(0, 6):\n",
    "            block = blocks[i]\n",
    "            block_dir = sessiondir + '/' + block\n",
    "\n",
    "            # in each block, get list of word pairs from first and second block\n",
    "            wordpairs = os.listdir(block_dir)\n",
    "\n",
    "            # within-groups analysis only has: SAME, REVERSE, DIFFERENT\n",
    "            diff_word_group = []\n",
    "            reverse_word_group = []\n",
    "            same_word_group = []\n",
    "\n",
    "            ################# 01: Create WordPair Groups #################\n",
    "            # create same group pairs\n",
    "            for idx, pair in enumerate(wordpairs):\n",
    "                same_word_group.append([pair, pair])\n",
    "\n",
    "            # create reverse, and different groups\n",
    "            for idx, pairs in enumerate(itertools.combinations(wordpairs,2)):\n",
    "                if isReverse(pairs[0], pairs[1]):\n",
    "                    reverse_word_group.append([pairs[0], pairs[1]])\n",
    "                else:\n",
    "                    diff_word_group.append([pairs[0], pairs[1]])\n",
    "\n",
    "            # print meta about this code\n",
    "            if debug_on:\n",
    "                print \"Analyzing block \", blocks[i]\n",
    "                print 'Subject: ', subj\n",
    "                print 'Session: ', session\n",
    "                print 'Block: ', block, '\\n'\n",
    "\n",
    "            ## EXTRACT DATA INTO A DICTIONARY \n",
    "            block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "    #         print block_data['data'].keys()\n",
    "    #         print block_data['meta']\n",
    "\n",
    "            # get the list of channels for this subject\n",
    "#             wordpair = block_data['data'].keys()[0]\n",
    "#             channels = block_data['data'][wordpair].keys()# channels\n",
    "\n",
    "            ################# 02a: Same Words Feature Construction #################\n",
    "            # extract channel data for same word group\n",
    "            for wdx, same_words in enumerate(same_word_group):\n",
    "                # extract data to process - average across time \n",
    "                same_word_key = same_words[0]\n",
    "\n",
    "                probeOnTime = block_data['data'][same_word_key][channel]['timeZero']\n",
    "                numevents = block_data['data'][same_word_key][channel]['powerMat'].shape[0]\n",
    "                if numevents % 2 != 0:\n",
    "                    block_data['data'][same_word_key][channel]['powerMat'] = block_data['data'][same_word_key][channel]['powerMat'][0:numevents-1,:,:]\n",
    "\n",
    "                ## split same_word data in within groups in half\n",
    "                vocalizationTime = block_data['data'][same_word_key][channel]['timeVocalization']\n",
    "                powerMat_first = block_data['data'][same_word_key][channel]['powerMat'][0:numevents/2,:,:]\n",
    "                powerMat_second = block_data['data'][same_word_key][channel]['powerMat'][numevents/2:numevents,:,:]\n",
    "                powerMat = powerMat_second - powerMat_first # get the difference in spectral power\n",
    "\n",
    "                # append each event\n",
    "                for i in range(0, numevents/2):\n",
    "                    # either go from timezero -> vocalization, or some other timewindow\n",
    "                    if same_word_data.size == 0:\n",
    "                        same_word_data = powerMat[i,:,probeOnTime:probeOnTime+6]\n",
    "                        same_word_data = np.reshape(same_word_data, (1, 7, 6))\n",
    "                    else:\n",
    "                        same_word_data = np.append(same_word_data, np.reshape(powerMat[i,:,probeOnTime:probeOnTime+6], (1,7,6)), axis=0)\n",
    "\n",
    "            ################# 02b: Different Words Feature Construction #################\n",
    "            for diff_words in diff_word_group:\n",
    "                # extract wordKey and data from MAIN block dictinoary\n",
    "                diff_word_one = diff_words[0]\n",
    "                diff_word_two = diff_words[1]\n",
    "\n",
    "                probeOnTime = block_data['data'][diff_word_one][channel]['timeZero']\n",
    "                vocalizationTime = block_data['data'][diff_word_one][channel]['timeVocalization']\n",
    "                powerMat_first = block_data['data'][diff_word_one][channel]['powerMat']\n",
    "                powerMat_second = block_data['data'][diff_word_two][channel]['powerMat']\n",
    "\n",
    "                # check events and get the number of events X frequency X time\n",
    "                numevents_first = powerMat_first.shape[0]\n",
    "                numevents_second = powerMat_second.shape[0]\n",
    "                numevents = min(numevents_first, numevents_second)\n",
    "                powerMat = powerMat_second[0:numevents,:,:] - powerMat_first[0:numevents,:,:]\n",
    "\n",
    "                # append each event\n",
    "                for i in range(0, numevents):\n",
    "                    # either go from timezero -> vocalization, or some other timewindow\n",
    "                    if diff_word_data.size == 0:\n",
    "                        diff_word_data = np.reshape(powerMat[i,:,probeOnTime:probeOnTime+6], (1,7,6))\n",
    "                    else:\n",
    "                        diff_word_data = np.append(diff_word_data, np.reshape(powerMat[i,:,probeOnTime:probeOnTime+6], (1,7,6)), axis=0)\n",
    "\n",
    "#                 break # only do 1 channel\n",
    "        #         break # 1 block\n",
    "        #     break # 1 session\n",
    "    \n",
    "    buff1 = np.mean(same_word_data[:,0:2,:], axis=1, keepdims=True)\n",
    "    buff2 = same_word_data[:,2:4,:]\n",
    "    buff3 = np.mean(same_word_data[:,4:,:], axis=1, keepdims=True)\n",
    "    same_word_data = np.concatenate((buff1, buff2, buff3), axis=1)\n",
    "    \n",
    "    buff1 = np.mean(diff_word_data[:,0:2,:], axis=1, keepdims=True)\n",
    "    buff2 = diff_word_data[:,2:4,:]\n",
    "    buff3 = np.mean(diff_word_data[:,4:,:], axis=1, keepdims=True)\n",
    "    diff_word_data = np.concatenate((buff1, buff2, buff3), axis=1)\n",
    "    \n",
    "    print same_word_data.shape\n",
    "    print diff_word_data.shape\n",
    "    \n",
    "    \n",
    "    ################# 03: Perform Logistic Regression and Feature Selection #################\n",
    "    # Create classes and feature vects\n",
    "    first_data = np.reshape(diff_word_data, (diff_word_data.shape[0], 24))\n",
    "    zero_data = np.reshape(same_word_data, (same_word_data.shape[0], 24))\n",
    "    \n",
    "    features = np.append(first_data, zero_data, axis=0)\n",
    "    y = np.ones((first_data.shape[0],))\n",
    "    y = np.concatenate((y, np.zeros((zero_data.shape[0],))))\n",
    "\n",
    "    print features.shape\n",
    "    print y.shape\n",
    "    \n",
    "    for idx, cla in enumerate(classifiers):\n",
    "        print names[idx]\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, y, test_size=0.4, random_state=0)\n",
    "\n",
    "        clf = cla.fit(X_train, y_train)\n",
    "        loo = LeaveOneOut(len(features))\n",
    "        scores = cross_validation.cross_val_score(clf, features, y, cv=loo)\n",
    "        baseaccuracy[cdx,] = [scores.mean(), scores.std()]\n",
    "\n",
    "        print \"Channel: \", channel\n",
    "        print baseaccuracy[cdx,]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2)\n",
      "0.656807751147\n",
      "(18, 5, 119)\n",
      "['delta_theta' 'alpha' 'beta' 'lowgamma_hfo']\n"
     ]
    }
   ],
   "source": [
    "print baseaccuracy.shape\n",
    "print 1288/1961.\n",
    "\n",
    "freq_labels = ['delta', 'theta', 'alpha', 'beta', 'low gamma', 'high gamma', 'HFO']\n",
    "\n",
    "test = powerMat[:,0:4,:]\n",
    "highbands = np.mean(powerMat[:,4:,:], axis=1, keepdims=True)\n",
    "lowbands = powerMat[:,0:4,:]\n",
    "highbands = np.mean(powerMat[:,4:,:], axis=1, keepdims=True)\n",
    "power_matrix = np.concatenate((lowbands, highbands), axis=1)\n",
    "print power_matrix.shape\n",
    "\n",
    "freq_bands = np.array(['delta_theta', 'alpha', 'beta', 'lowgamma_hfo'])\n",
    "print freq_bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feature Selection\n",
    "\n",
    "Now we have a base accuracy for each channel. We can start removing features and see how the accuracy changes. This will give us an idea of the importance of a certain frequency band (delta -> HFO).\n",
    "\n",
    "Delta->Theta, Alpha, Beta, LowGamma->HFO\n",
    "\n",
    "Store a dictionary for j=1,...,4 feature vectors and \n",
    "Dictionary[1] = scores for all the combinations\n",
    "\n",
    "Reference: https://www.cs.cmu.edu/~kdeng/thesis/feature.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1288, 4, 6)\n",
      "(673, 4, 6)\n",
      "(1961, 4, 6)\n",
      "98\n",
      "[[0, 1, 2, 3, 4, 5]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11]]\n",
      "(1304, 24)\n",
      "[[12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[{'alpha': [0.64493865030674846, 0.47853190870333612], 'beta': [0.64493865030674846, 0.47853190870333612], 'delta_theta': [0.64493865030674846, 0.47853190870333612], 'lowgamma_hfo': [0.64340490797546013, 0.4789937706991082]}, {'delta_theta&lowgamma_hfo': [0.6426380368098159, 0.47922269401087592], 'alpha&lowgamma_hfo': [0.64187116564417179, 0.47945028142526308], 'beta&lowgamma_hfo': [0.6426380368098159, 0.47922269401087586], 'delta_theta&beta': [0.64340490797546013, 0.47899377069910815], 'alpha&beta': [0.64493865030674846, 0.47853190870333612], 'delta_theta&alpha': [0.6426380368098159, 0.47922269401087586]}, {'delta_theta&alpha&lowgamma_hfo': [0.64187116564417179, 0.47945028142526308], 'alpha&beta&lowgamma_hfo': [0.64110429447852757, 0.47967653484376005], 'delta_theta&alpha&beta': [0.64340490797546013, 0.47899377069910815], 'delta_theta&beta&lowgamma_hfo': [0.64340490797546013, 0.47899377069910815]}]\n",
      "(98, 4, 6)\n",
      "(1863, 4, 6)\n",
      "(559, 4, 6)\n",
      "(1304, 4, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:54: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "np.random.seed(123456789)  # for reproducibility, set random seed\n",
    "\n",
    "####### Function to build up that performs forward feature selection of X\n",
    "# Input: - X: with samples X feature vectors\n",
    "# - Y: the vector of binary labels (0, 1)\n",
    "def forwardFeatureSelection(X1, X2, Y):\n",
    "    print X.shape\n",
    "    print Y.shape\n",
    "\n",
    "################# 03: Perform Logistic Regression and Feature Selection #################\n",
    "print diff_word_data.shape\n",
    "print same_word_data.shape\n",
    "\n",
    "y = np.ones((same_word_data.shape[0],))\n",
    "y = np.concatenate((y, np.zeros((diff_word_data.shape[0],))))\n",
    "\n",
    "X = np.concatenate((same_word_data, diff_word_data), axis=0)\n",
    "numPartitions = 20\n",
    "partitionLength = X.shape[0]/numPartitions\n",
    "print X.shape\n",
    "print X.shape[0]/numPartitions\n",
    "\n",
    "## Shuffle feature set and labels\n",
    "c = np.c_[X.reshape(len(X), -1), y.reshape(len(y), -1)]\n",
    "X2 = c[:, :X.size//len(X)].reshape(X.shape)\n",
    "y2 = c[:, X.size//len(X)].reshape(y.shape)\n",
    "np.random.shuffle(c)\n",
    "\n",
    "m = 4 # number of attributes \n",
    "# loop through all partitions of dataset\n",
    "indice_range = range(0, len(y2))\n",
    "for i in range(0, numPartitions):\n",
    "    score_dict = []\n",
    "    max_score = 0\n",
    "    \n",
    "    ## 01: set outer test set and labels\n",
    "    if i==numPartitions-1:\n",
    "        indices = i*partitionLength\n",
    "        outertestset = X2[indices:,:,:]\n",
    "        outertestlabels = y2[indices:]\n",
    "    else:\n",
    "        indices = range(i*partitionLength,(i+1)*partitionLength)\n",
    "        outertestset = X2[indices,:,:]\n",
    "        outertestlabels = y2[indices]\n",
    "        \n",
    "    ## 02: Create outertrainset\n",
    "    indices = list(set(indice_range) - set(indices))\n",
    "    outertrainset = X2[indices,:,:]\n",
    "    outertrainlabels = y2[indices]\n",
    "    \n",
    "    ## 03: Create innertrainset and innertestset\n",
    "    # create a random 70% of the indices\n",
    "    indices = np.random.choice(range(outertrainlabels.shape[0]), size=np.floor(outertrainlabels.shape[0]*0.7), replace=False)\n",
    "    innertrainset = outertrainset[indices,:,:]\n",
    "    innertrainlabels = outertrainlabels[indices]\n",
    "    \n",
    "    # innertestset created from the other 30% of the partition\n",
    "    indice_range = range(0, outertrainlabels.shape[0])\n",
    "    indices = list(set(indice_range) - set(indices))\n",
    "    innertestset = outertrainset[indices,:,:]\n",
    "    innertestlabels = outertrainlabels[indices]\n",
    "    \n",
    "    ## Search for best feature set through all combinations of m\n",
    "    features = np.reshape(innertrainset, (innertrainset.shape[0], 6*4))\n",
    "    y = innertrainlabels\n",
    "    testrange = np.arange(0,4,1) # test range to provide itertools with comb of #'s\n",
    "    for j in range(1, m):\n",
    "        score_dict.append({})\n",
    "        combs = list(itertools.combinations(testrange,j))\n",
    "        for cdx, comb in enumerate(combs):\n",
    "            indices = [range(comb[i]*6, comb[i]*6+6) for i in range(0, len(comb))]\n",
    "            print indices\n",
    "            print features.shape\n",
    "            new_features = np.reshape(features[:, indices], (features.shape[0], 6*len(comb)))\n",
    "\n",
    "            ## perform leave one out\n",
    "            clf = LogisticRegression().fit(new_features, y)\n",
    "            loo = LeaveOneOut(len(new_features))\n",
    "            scores = cross_validation.cross_val_score(clf, new_features, y, cv=loo)\n",
    "            \n",
    "            ## Store performance in dictionary\n",
    "            if len(comb) == 1:\n",
    "                key = freq_bands[comb[0]]\n",
    "            elif len(comb) == 2:\n",
    "                key = freq_bands[comb[0]] + '&' + freq_bands[comb[1]] \n",
    "            elif len(comb) == 3:\n",
    "                key = freq_bands[comb[0]] + '&' + freq_bands[comb[1]] + '&' + freq_bands[comb[2]] \n",
    "            elif len(comb) == 4:\n",
    "                key = 'all'\n",
    "            \n",
    "            if max_score < scores.mean():\n",
    "                max_score = max(scores.mean(), max_score)\n",
    "                max_id = comb\n",
    "            \n",
    "            score_dict[j-1][key] = [scores.mean(), scores.std()]\n",
    "#             break\n",
    "#         break\n",
    "    print score_dict\n",
    "    print outertestset.shape\n",
    "    print outertrainset.shape\n",
    "    print innertestset.shape\n",
    "    print innertrainset.shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1961, 24)\n",
      "(0, 1, 2)\n",
      "(0, 1, 3)\n",
      "(0, 2, 3)\n",
      "(1, 2, 3)\n",
      "{'delta_theta&alpha&lowgamma_hfo': 'test', 'alpha&beta&lowgamma_hfo': 'test', 'delta_theta&alpha&beta': 'test', 'delta_theta&beta&lowgamma_hfo': 'test'}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "test = np.arange(0, 4, 1)\n",
    "combs = list(itertools.combinations(test,3))\n",
    "print features.shape\n",
    "\n",
    "testdict = {}\n",
    "for cdx, comb in enumerate(combs):\n",
    "    indices = [range(comb[i]*6, comb[i]*6+6) for i in range(0, len(comb))]\n",
    "    test = np.reshape(features[:, indices], (features.shape[0], 6*len(comb)))\n",
    "    if len(comb) == 1:\n",
    "        key = freq_bands[comb[0]]\n",
    "    elif len(comb) == 2:\n",
    "        key = freq_bands[comb[0]] + '&' + freq_bands[comb[1]] \n",
    "    elif len(comb) == 3:\n",
    "        key = freq_bands[comb[0]] + '&' + freq_bands[comb[1]] + '&' + freq_bands[comb[2]] \n",
    "    elif len(comb) == 4:\n",
    "        key = 'all'\n",
    "#     key = freq_bands[]\n",
    "#     print key\n",
    "    testdict[key] = 'test'\n",
    "#     print test.shape\n",
    "    print comb\n",
    "    \n",
    "print testdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'alpha': [0.64493865030674846, 0.47853190870333612],\n",
      "  'beta': [0.64493865030674846, 0.47853190870333612],\n",
      "  'delta_theta': [0.64493865030674846, 0.47853190870333612],\n",
      "  'lowgamma_hfo': [0.64340490797546013, 0.4789937706991082]},\n",
      " {'alpha&beta': [0.64493865030674846, 0.47853190870333612],\n",
      "  'alpha&lowgamma_hfo': [0.64187116564417179, 0.47945028142526308],\n",
      "  'beta&lowgamma_hfo': [0.6426380368098159, 0.47922269401087586],\n",
      "  'delta_theta&alpha': [0.6426380368098159, 0.47922269401087586],\n",
      "  'delta_theta&beta': [0.64340490797546013, 0.47899377069910815],\n",
      "  'delta_theta&lowgamma_hfo': [0.6426380368098159, 0.47922269401087592]},\n",
      " {'alpha&beta&lowgamma_hfo': [0.64110429447852757, 0.47967653484376005],\n",
      "  'delta_theta&alpha&beta': [0.64340490797546013, 0.47899377069910815],\n",
      "  'delta_theta&alpha&lowgamma_hfo': [0.64187116564417179,\n",
      "                                     0.47945028142526308],\n",
      "  'delta_theta&beta&lowgamma_hfo': [0.64340490797546013, 0.47899377069910815]}]\n",
      "(0,)\n",
      "0.644938650307\n",
      "[ 0.  1.  0. ...,  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(score_dict)\n",
    "print max_id\n",
    "print max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345678)  # for reproducibility, set random seed\n",
    "\n",
    "names = [\"Linear SVM\", \n",
    "         \"Random Forest\",\n",
    "         \"Quadratic Discriminant Analysis\",\n",
    "        \"Logistic Regression\"]\n",
    "\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1288, 4, 6)\n",
      "(673, 4, 6)\n",
      "(1961, 4, 6)\n",
      "98\n",
      "[[0, 1, 2, 3, 4, 5]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11]]\n",
      "(1304, 24)\n",
      "[[12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[0, 1, 2, 3, 4, 5], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[[6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]\n",
      "(1304, 24)\n",
      "[{'alpha': [0.64877300613496935, 0.4773537395323979], 'beta': [0.64033742331288346, 0.47990145615313629], 'delta_theta': [0.6380368098159509, 0.48056824607524268], 'lowgamma_hfo': [0.63880368098159512, 0.48034730991852093]}, {'delta_theta&lowgamma_hfo': [0.63650306748466257, 0.48100614608056486], 'alpha&lowgamma_hfo': [0.64570552147239269, 0.47829896614194989], 'beta&lowgamma_hfo': [0.6426380368098159, 0.47922269401087586], 'delta_theta&beta': [0.63113496932515334, 0.48249727441726664], 'alpha&beta': [0.64800613496932513, 0.47759206862283832], 'delta_theta&alpha': [0.64110429447852757, 0.47967653484376005]}, {'delta_theta&alpha&lowgamma_hfo': [0.64110429447852757, 0.47967653484376005], 'alpha&beta&lowgamma_hfo': [0.64033742331288346, 0.47990145615313623], 'delta_theta&alpha&beta': [0.64723926380368102, 0.47782904808576676], 'delta_theta&beta&lowgamma_hfo': [0.6464723926380368, 0.47806467992823815]}]\n",
      "(98, 4, 6)\n",
      "(1863, 4, 6)\n",
      "(559, 4, 6)\n",
      "(1304, 4, 6)\n",
      "(1,)\n",
      "0.648773006135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:54: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "np.random.seed(123456789)  # for reproducibility, set random seed\n",
    "\n",
    "####### Function to build up that performs forward feature selection of X\n",
    "# Input: - X: with samples X feature vectors\n",
    "# - Y: the vector of binary labels (0, 1)\n",
    "def forwardFeatureSelection(X1, X2, Y):\n",
    "    print X.shape\n",
    "    print Y.shape\n",
    "\n",
    "################# 03: Perform Logistic Regression and Feature Selection #################\n",
    "print diff_word_data.shape\n",
    "print same_word_data.shape\n",
    "\n",
    "y = np.ones((same_word_data.shape[0],))\n",
    "y = np.concatenate((y, np.zeros((diff_word_data.shape[0],))))\n",
    "\n",
    "X = np.concatenate((same_word_data, diff_word_data), axis=0)\n",
    "numPartitions = 20\n",
    "partitionLength = X.shape[0]/numPartitions\n",
    "print X.shape\n",
    "print X.shape[0]/numPartitions\n",
    "\n",
    "## Shuffle feature set and labels\n",
    "c = np.c_[X.reshape(len(X), -1), y.reshape(len(y), -1)]\n",
    "X2 = c[:, :X.size//len(X)].reshape(X.shape)\n",
    "y2 = c[:, X.size//len(X)].reshape(y.shape)\n",
    "np.random.shuffle(c)\n",
    "\n",
    "m = 4 # number of attributes \n",
    "# loop through all partitions of dataset\n",
    "indice_range = range(0, len(y2))\n",
    "for i in range(0, numPartitions):\n",
    "    score_dict = []\n",
    "    max_score = 0\n",
    "    \n",
    "    ## 01: set outer test set and labels\n",
    "    if i==numPartitions-1:\n",
    "        indices = i*partitionLength\n",
    "        outertestset = X2[indices:,:,:]\n",
    "        outertestlabels = y2[indices:]\n",
    "    else:\n",
    "        indices = range(i*partitionLength,(i+1)*partitionLength)\n",
    "        outertestset = X2[indices,:,:]\n",
    "        outertestlabels = y2[indices]\n",
    "        \n",
    "    ## 02: Create outertrainset\n",
    "    indices = list(set(indice_range) - set(indices))\n",
    "    outertrainset = X2[indices,:,:]\n",
    "    outertrainlabels = y2[indices]\n",
    "    \n",
    "    ## 03: Create innertrainset and innertestset\n",
    "    # create a random 70% of the indices\n",
    "    indices = np.random.choice(range(outertrainlabels.shape[0]), size=np.floor(outertrainlabels.shape[0]*0.7), replace=False)\n",
    "    innertrainset = outertrainset[indices,:,:]\n",
    "    innertrainlabels = outertrainlabels[indices]\n",
    "    \n",
    "    # innertestset created from the other 30% of the partition\n",
    "    indice_range = range(0, outertrainlabels.shape[0])\n",
    "    indices = list(set(indice_range) - set(indices))\n",
    "    innertestset = outertrainset[indices,:,:]\n",
    "    innertestlabels = outertrainlabels[indices]\n",
    "    \n",
    "    ## Search for best feature set through all combinations of m\n",
    "    features = np.reshape(innertrainset, (innertrainset.shape[0], 6*4))\n",
    "    y = innertrainlabels\n",
    "    testrange = np.arange(0,4,1) # test range to provide itertools with comb of #'s\n",
    "    for j in range(1, m):\n",
    "        score_dict.append({})\n",
    "        combs = list(itertools.combinations(testrange,j))\n",
    "        for cdx, comb in enumerate(combs):\n",
    "            indices = [range(comb[i]*6, comb[i]*6+6) for i in range(0, len(comb))]\n",
    "            print indices\n",
    "            print features.shape\n",
    "            new_features = np.reshape(features[:, indices], (features.shape[0], 6*len(comb)))\n",
    "\n",
    "            ## perform leave one out\n",
    "            clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1).fit(new_features, y)\n",
    "            loo = LeaveOneOut(len(new_features))\n",
    "            scores = cross_validation.cross_val_score(clf, new_features, y, cv=loo)\n",
    "            \n",
    "            ## Store performance in dictionary\n",
    "            if len(comb) == 1:\n",
    "                key = freq_bands[comb[0]]\n",
    "            elif len(comb) == 2:\n",
    "                key = freq_bands[comb[0]] + '&' + freq_bands[comb[1]] \n",
    "            elif len(comb) == 3:\n",
    "                key = freq_bands[comb[0]] + '&' + freq_bands[comb[1]] + '&' + freq_bands[comb[2]] \n",
    "            elif len(comb) == 4:\n",
    "                key = 'all'\n",
    "            \n",
    "            if max_score < scores.mean():\n",
    "                max_score = max(scores.mean(), max_score)\n",
    "                max_id = comb\n",
    "            \n",
    "            score_dict[j-1][key] = [scores.mean(), scores.std()]\n",
    "#             break\n",
    "#         break\n",
    "    print score_dict\n",
    "    print outertestset.shape\n",
    "    print outertrainset.shape\n",
    "    print innertestset.shape\n",
    "    print innertrainset.shape\n",
    "    break\n",
    "    \n",
    "print max_id\n",
    "print max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
