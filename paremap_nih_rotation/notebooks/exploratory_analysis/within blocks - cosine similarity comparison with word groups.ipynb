{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting and Comparing Cosine Similarity Distributions\n",
    "\n",
    "By: Adam Li\n",
    "\n",
    "Here I extract from every channel a 7x1 frequency vector from 0.5 seconds before vocalization to vocalization (time averaged). This results in a #channelsX7 feature vector for every word pair event. \n",
    "\n",
    "I compare same word pairings vs. different word pairings (e.g. BRICK_CLOCK vs BRICK_CLOCK and BRICK_CLOCK vs GLASS_JUICE) and produce the following:\n",
    "\n",
    "1. Plot Histograms of cosine similarities\n",
    "2. t-test or ks test between the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial import distance as Distance\n",
    "\n",
    "# pretty charting\n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low bands:  ['delta', 'theta']\n",
      "high bands:  ['beta', 'low gamma', 'high gamma', 'HFO']\n",
      "The length of the feature vector for each channel will be:  70  total= 6720\n"
     ]
    }
   ],
   "source": [
    "################################### HYPER-PARAMETERS TO TUNE #######################################################\n",
    "np.random.seed(123456789)  # for reproducibility, set random seed\n",
    "distances = Distance.cosine # define distance metric to use\n",
    "num_time_windows = 10\n",
    "low_freq_bands = [0, 1]\n",
    "high_freq_bands = [3, 4, 5, 6]\n",
    "freq_bands = np.arange(0,7,1)\n",
    "\n",
    "freq_labels = ['delta', 'theta', 'alpha', 'beta', 'low gamma', 'high gamma', 'HFO']\n",
    "# print freq_bands\n",
    "# print [freq_labels[i] for i in freq_bands]\n",
    "\n",
    "print 'low bands: ', [freq_labels[i] for i in low_freq_bands]\n",
    "print 'high bands: ', [freq_labels[i] for i in high_freq_bands]\n",
    "print \"The length of the feature vector for each channel will be: \", num_time_windows*len(freq_bands), \\\n",
    "            ' total=', 96*num_time_windows*len(freq_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Extract wordpairs data into a dictionary for a subject/session/block\n",
    "#### dictionary{wordpair:{channels}}\n",
    "def extractSubjSessionBlockData(subj, session, block):\n",
    "    # file directory for a subj/session/block\n",
    "    filedir = '../../condensed_data_' + subj + '/sessions_05122016/' + session + '/' + block\n",
    "    wordpairs = os.listdir(filedir) \n",
    "    \n",
    "    # initialize data dictionary with meta data\n",
    "    data_dict = {}\n",
    "    data_dict['meta'] = {'subject': subj,\n",
    "                         'session': session,\n",
    "                         'block': block}\n",
    "    data_dict['data'] = {}\n",
    "    for wordpair in wordpairs:    # loop thru all wordpairs\n",
    "        wordpair_dir = filedir + '/' + wordpair\n",
    "        all_channel_mats = os.listdir(wordpair_dir)\n",
    "        \n",
    "        data_dict['data'][wordpair] = {}\n",
    "        for channel in all_channel_mats: # loop thru all channels\n",
    "            chan_file = wordpair_dir + '/' + channel\n",
    "\n",
    "            ## 00: load in data\n",
    "            data = scipy.io.loadmat(chan_file)\n",
    "            data = data['data']\n",
    "            \n",
    "            ## 01: get the time point for probeword on\n",
    "            timeZero = data['timeZero'][0][0][0]\n",
    "        \n",
    "            ## 02: get the time point of vocalization\n",
    "            vocalization = data['vocalization'][0][0][0]\n",
    "        \n",
    "            ## 03: Get Power Matrix\n",
    "            power_matrix = data['powerMatZ'][0][0]\n",
    "            \n",
    "            chan = channel.split('_')[0]\n",
    "            \n",
    "            # convert channel data into a json dict\n",
    "            data_dict['data'][wordpair][chan] = {'timeZero': timeZero,\n",
    "                                          'timeVocalization':vocalization,\n",
    "                                          'powerMat': power_matrix}\n",
    "            \n",
    "    return data_dict\n",
    "\n",
    "def isReverse(pair1, pair2):\n",
    "    pair1split = pair1.split('_')\n",
    "    pair2split = pair2.split('_')\n",
    "    if pair1split[0] == pair2split[1] and pair1split[1] == pair2split[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Compute all pairwise distances between first_mat to second_mat\n",
    "def computePairDistances(first_mat, second_mat):\n",
    "    distance_list = []\n",
    "    for idx in range(0, first_mat.shape[0]):\n",
    "        distance_list.append([distances(x, first_mat[idx,:]) for x in second_mat])\n",
    "    distance_list = 1.0 - np.ndarray.flatten(np.array(distance_list))\n",
    "    return distance_list  \n",
    "    \n",
    "distances = Distance.cosine # define distance metric to use\n",
    "def computeWithinDistances(mat):\n",
    "    distance_list = np.array(())\n",
    "    \n",
    "    distance_list = []\n",
    "    for idx in range(0, mat.shape[0]):\n",
    "        for x in mat[idx+1:,:]:\n",
    "            dist = distances(x,mat[idx,:])\n",
    "            to_append = np.array(dist)\n",
    "            distance_list.append(to_append)\n",
    "            \n",
    "    distance_list = 1.0 - np.ndarray.flatten(np.array(distance_list))\n",
    "    return distance_list\n",
    "\n",
    "def createWordGroups(wordpairs):\n",
    "    same_word_group = []\n",
    "    reverse_word_group = []\n",
    "    diff_word_group = []\n",
    "    # create same group pairs\n",
    "    for idx, pair in enumerate(wordpairs):\n",
    "        same_word_group.append([pair, pair])\n",
    "\n",
    "    # create reverse, and different groups\n",
    "    for idx, pairs in enumerate(itertools.combinations(wordpairs,2)):\n",
    "        if isReverse(pairs[0], pairs[1]):\n",
    "            reverse_word_group.append([pairs[0], pairs[1]])\n",
    "        else:\n",
    "            diff_word_group.append([pairs[0], pairs[1]])\n",
    "            \n",
    "    return same_word_group, reverse_word_group, diff_word_group\n",
    "\n",
    "def plotDescription(session, block):\n",
    "    fig=plt.figure()\n",
    "    axes = plt.gca()\n",
    "    ymin, ymax = axes.get_ylim()\n",
    "    xmin, xmax = axes.get_xlim()\n",
    "    plt.text((xmax-xmin)/4.5, (ymax-ymin)/2, r'Session %s %scomparing %s'%(session, '\\n',block), fontsize=20)\n",
    "    plt.title(session + ' ' + block + ' within-block analysis')\n",
    "    plt.grid(False)\n",
    "\n",
    "def plotHistogramDistances(distances, session, block, wordpairtype):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.grid(False)\n",
    "    plt.hist(distances, color='k', lw=3)\n",
    "    plt.xlabel('Cosine Similarity n='+str(len(distances)))\n",
    "    plt.ylabel('Frequency Count')\n",
    "    plt.title(wordpairtype + ' Within-block pairwise distances in ' + session + ' with ' + block)\n",
    "    plt.xlim([-1,1])\n",
    "    plt.legend(r'n= %s'%(str(len(distances))))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def createSameWordSimilarities(block_data, channels):\n",
    "    same_word_distances = np.array(())\n",
    "    \n",
    "    ################# 02a: Same Words Cosine Distnace #################\n",
    "    # extract channel data for same word group\n",
    "    same_word_dict = {}\n",
    "    for same_words in same_word_group:\n",
    "        same_feature_mat = np.array(()) # initialize matrix to store feature vectors for each event\n",
    "\n",
    "        for chan in channels: \n",
    "            ## 01: extract data to process - average across time \n",
    "            same_word_data = []\n",
    "            same_word_key = same_words[0]\n",
    "            probeOnTime = block_data['data'][same_word_key][str(chan)]['timeZero']\n",
    "            vocalizationTime = block_data['data'][same_word_key][str(chan)]['timeVocalization']\n",
    "            powerMat = block_data['data'][same_word_key][str(chan)]['powerMat']\n",
    "\n",
    "            ## 02: average across time and append frequency feature vector\n",
    "            for i in range(0, len(vocalizationTime)):\n",
    "                # either go from timezero -> vocalization, or some other timewindow\n",
    "                feature_vect = np.mean(powerMat[i,freq_bands,vocalizationTime[i]-num_time_windows:vocalizationTime[i]-1],axis=1)\n",
    "                same_word_data.append(np.ndarray.flatten(feature_vect))\n",
    "\n",
    "            ## 03: append freq. feature vector per channel to each event\n",
    "            if same_feature_mat.size == 0:\n",
    "                same_feature_mat = np.array(same_word_data)\n",
    "            else:\n",
    "                same_feature_mat = np.append(same_feature_mat, np.array(same_word_data), axis=1)\n",
    "\n",
    "        ## 04: do a pairwise comparison of all events in this word pair\n",
    "        same_feature_mat = computeWithinDistances(same_feature_mat)\n",
    "        same_word_dict[same_word_key] = same_feature_mat\n",
    "\n",
    "    ## 05: convert into list of distances\n",
    "    for key in same_word_dict.keys():\n",
    "        if same_word_distances.size == 0:\n",
    "            same_word_distances = same_word_dict[key]\n",
    "        else:\n",
    "            same_word_distances = np.append(same_word_distances, same_word_dict[key], axis=0)\n",
    "    return same_word_distances\n",
    "\n",
    "def createDiffWordSimilarities(block_data, channels):\n",
    "    diff_word_distances = np.array(())\n",
    "   \n",
    "    ########################## 02b: DIFFERENT WORD PAIRS ########################################\n",
    "    diff_word_dict = {}\n",
    "    for diff_words in diff_word_group:\n",
    "        diff_feature_mat1 = np.array(()) # initialize matrix to store feature vectors for each event\n",
    "        diff_feature_mat2 = np.array(())\n",
    "\n",
    "        # keys for different words\n",
    "        diff_word1 = diff_words[0]\n",
    "        diff_word2 = diff_words[1] \n",
    "\n",
    "        for chan in channels:    # loop through every channel\n",
    "            # word keys and buffer lists\n",
    "            diff_word_databuffer1 = []\n",
    "            diff_word_databuffer2 = []\n",
    "\n",
    "            ## 01: extract channel data from blockdata dictionary\n",
    "            probeOnTime = block_data['data'][diff_word1][str(chan)]['timeZero']\n",
    "            vocalizationTime = block_data['data'][diff_word1][str(chan)]['timeVocalization']\n",
    "            powerMat = block_data['data'][diff_word1][str(chan)]['powerMat']\n",
    "            ## 02: average across time and append frequency feature vector\n",
    "            for i in range(0, len(vocalizationTime)):\n",
    "                feature_vect = np.mean(powerMat[i,freq_bands,vocalizationTime[i]-num_time_windows:vocalizationTime[i]-1],axis=1)\n",
    "                diff_word_databuffer1.append(np.ndarray.flatten(feature_vect))\n",
    "            ## 03:append freq. feature vector per channel to each event\n",
    "            if diff_feature_mat1.size == 0:\n",
    "                diff_feature_mat1 = np.array(diff_word_databuffer1)\n",
    "            else:\n",
    "                diff_feature_mat1 = np.append(diff_feature_mat1, np.array(diff_word_databuffer1), axis=1)\n",
    "\n",
    "            probeOnTime = block_data['data'][diff_word2][str(chan)]['timeZero']\n",
    "            vocalizationTime = block_data['data'][diff_word2][str(chan)]['timeVocalization']\n",
    "            powerMat = block_data['data'][diff_word2][str(chan)]['powerMat']\n",
    "            ## 02: average across time and append frequency feature vector\n",
    "            for i in range(0, len(vocalizationTime)):\n",
    "                feature_vect = np.mean(powerMat[i,freq_bands,vocalizationTime[i]-num_time_windows:vocalizationTime[i]-1],axis=1)\n",
    "                diff_word_databuffer2.append(np.ndarray.flatten(feature_vect))\n",
    "            ## 03:append freq. feature vector per channel to each event\n",
    "            if diff_feature_mat2.size == 0:\n",
    "                diff_feature_mat2 = np.array(diff_word_databuffer2)\n",
    "            else:\n",
    "                diff_feature_mat2 = np.append(diff_feature_mat2, np.array(diff_word_databuffer2), axis=1)\n",
    "\n",
    "        ## 04: pairwise comparison\n",
    "        diff_word_dict['vs'.join(diff_words)] = computePairDistances(diff_feature_mat1, diff_feature_mat2)\n",
    "\n",
    "    # convert into list of distances\n",
    "    for key in diff_word_dict.keys():\n",
    "        if diff_word_distances.size == 0:\n",
    "            diff_word_distances = diff_word_dict[key]\n",
    "        else:\n",
    "            diff_word_distances = np.append(diff_word_distances, diff_word_dict[key], axis=0)\n",
    "    return diff_word_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing session  session_1\n",
      "\n",
      "On block:  BLOCK_0\n",
      "['BRICK_CLOCK', 'CLOCK_BRICK', 'GLASS_JUICE', 'JUICE_GLASS']\n",
      "same word group: \n",
      "[['BRICK_CLOCK', 'BRICK_CLOCK'], ['CLOCK_BRICK', 'CLOCK_BRICK'], ['GLASS_JUICE', 'GLASS_JUICE'], ['JUICE_GLASS', 'JUICE_GLASS']] \n",
      "\n",
      "diff word group: \n",
      "[['BRICK_CLOCK', 'GLASS_JUICE'], ['BRICK_CLOCK', 'JUICE_GLASS'], ['CLOCK_BRICK', 'GLASS_JUICE'], ['CLOCK_BRICK', 'JUICE_GLASS']]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH034'\n",
    "filedir = '../../condensed_data_' + subj + '/sessions_05122016/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:]\n",
    "num_chans = 96\n",
    "debug_on = 0\n",
    "\n",
    "# loop through each session\n",
    "for session in sessions:\n",
    "    print \"Analyzing session \", session\n",
    "    sessiondir = filedir + session\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "\n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 6):\n",
    "#         print \"Analyzing block \", blocks[i]\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "        \n",
    "        ## 01: Create WordPair Groups\n",
    "        same_word_group, reverse_word_group, diff_word_group = createWordGroups(wordpairs)\n",
    "        \n",
    "        print \"\\nOn block: \", block\n",
    "        print wordpairs\n",
    "        print \"same word group: \\n\", same_word_group, '\\n'\n",
    "        print \"diff word group: \\n\", diff_word_group\n",
    "        print len(same_word_group)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '../../condensed_data_NIH034/sessions05122016/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a0f7fa1c20d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NIH034'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfiledir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../condensed_data_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/sessions05122016/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiledir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_chans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '../../condensed_data_NIH034/sessions05122016/'"
     ]
    }
   ],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH034'\n",
    "filedir = '../../condensed_data_' + subj + '/sessions05122016/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:]\n",
    "num_chans = 96\n",
    "debug_on = 0\n",
    "\n",
    "# loop through each session\n",
    "for session in sessions:\n",
    "    print \"Analyzing session \", session\n",
    "    sessiondir = filedir + session\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "\n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 6):\n",
    "#         print \"Analyzing block \", blocks[i]\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "       \n",
    "        # within-groups analysis only has: SAME, REVERSE, DIFFERENT        \n",
    "        ## 01: Create WordPair Groups\n",
    "        same_word_group, reverse_word_group, diff_word_group = createWordGroups(wordpairs)\n",
    "        \n",
    "        #### plot meta information about which session and blocks we're analyzing\n",
    "        plotDescription(session, block)\n",
    "        \n",
    "        ## 02: extract sessionblockdata dictionary for all channels\n",
    "        block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "        channels = np.arange(1, num_chans+1, 1)\n",
    "        \n",
    "        same_word_distances = createSameWordSimilarities(block_data, channels)\n",
    "        diff_word_distances = createDiffWordSimilarities(block_data, channels)\n",
    "        \n",
    "        if debug_on:\n",
    "            print \"done...\"\n",
    "            print \"Example paired word feature matrix: \", diff_feature_mat2.shape\n",
    "            print same_word_distances.shape\n",
    "            print diff_word_distances.shape\n",
    "        \n",
    "        ##### 01: Plot Histogram of Cosine Similarities\n",
    "        plotHistogramDistances(same_word_distances, session, block, 'same word pairs')\n",
    "        plotHistogramDistances(diff_word_distances, session, block, 'diff word pairs')\n",
    "        \n",
    "        ##### RUN STATS COMPARISONS ON SAME VS. REVERSE, SAME VS. DIFF, \n",
    "        random_subset = np.random.choice(range(same_word_distances.shape[0]), size=len(same_word_distances)/2, replace=False)\n",
    "        random_subset2 = list(set(np.arange(0, len(same_word_distances))) - set(random_subset))\n",
    "        same_X = same_word_distances[random_subset]\n",
    "        same_Y = same_word_distances[random_subset2]\n",
    "        \n",
    "        # sub-sample the diff_word_distances if using ks_test\n",
    "#         stat, same_p_val = stats.ks_2samp(same_X, same_Y)\n",
    "#         stat, diff_p_val = stats.ks_2samp(same_word_distances, diff_word_distances)\n",
    "           \n",
    "        stat, same_p_val = stats.ttest_ind(same_X, same_Y)\n",
    "        stat, diff_p_val = stats.ttest_ind(same_word_distances, diff_word_distances)\n",
    "        \n",
    "        print \"On block: \", block, \" using t-test\"\n",
    "        print \"Same avg +/- std: %0.3f\" %np.mean(same_word_distances), ' +/- %0.3f' %stats.sem(same_word_distances)\n",
    "        print \"Different avg +/- std: %0.3f\" %np.mean(diff_word_distances), ' +/- %0.3f' %stats.sem(diff_word_distances)\n",
    "        print \"Same vs. Same comparison: %0.3f\" %same_p_val\n",
    "        print \"Same vs. Different Comparison: %0.3f\" %diff_p_val, \"\\n\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion On NIH034\n",
    "\n",
    "\n",
    "\n",
    "# Do NIH039 Now Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Extract wordpairs data into a dictionary for a subject/session/block\n",
    "#### dictionary{wordpair:{channels}}\n",
    "def extractSubjSessionBlockData(subj, session, block):\n",
    "    # file directory for a subj/session/block\n",
    "    filedir = '../../condensed_data_' + subj + '/sessions/' + session + '/' + block\n",
    "    wordpairs = os.listdir(filedir) \n",
    "    \n",
    "    # initialize data dictionary with meta data\n",
    "    data_dict = {}\n",
    "    data_dict['meta'] = {'subject': subj,\n",
    "                         'session': session,\n",
    "                         'block': block}\n",
    "    data_dict['data'] = {}\n",
    "    for wordpair in wordpairs:    # loop thru all wordpairs\n",
    "        wordpair_dir = filedir + '/' + wordpair\n",
    "        all_channel_mats = os.listdir(wordpair_dir)\n",
    "        \n",
    "        data_dict['data'][wordpair] = {}\n",
    "        for channel in all_channel_mats: # loop thru all channels\n",
    "            chan_file = wordpair_dir + '/' + channel\n",
    "\n",
    "            ## 00: load in data\n",
    "            data = scipy.io.loadmat(chan_file)\n",
    "            data = data['data']\n",
    "            \n",
    "            ## 01: get the time point for probeword on\n",
    "            timeZero = data['timeZero'][0][0][0]\n",
    "        \n",
    "            ## 02: get the time point of vocalization\n",
    "            vocalization = data['vocalization'][0][0][0]\n",
    "        \n",
    "            ## 03: Get Power Matrix\n",
    "            power_matrix = data['powerMatZ'][0][0]\n",
    "            \n",
    "            chan = channel.split('_')[0]\n",
    "            \n",
    "            # convert channel data into a json dict\n",
    "            data_dict['data'][wordpair][chan] = {'timeZero': timeZero,\n",
    "                                          'timeVocalization':vocalization,\n",
    "                                          'powerMat': power_matrix}\n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH039'\n",
    "filedir = '../../condensed_data_' + subj + '/sessions/'\n",
    "sessions = os.listdir(filedir)\n",
    "# sessions = sessions[2:]\n",
    "num_chans = 72\n",
    "debug_on = 0\n",
    "\n",
    "print \"Analyzing subject: \", subj, \" in \", filedir\n",
    "# loop through each session\n",
    "for session in sessions:\n",
    "    print \"Analyzing session \", session\n",
    "    sessiondir = filedir + session\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "\n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 6):\n",
    "#         print \"Analyzing block \", blocks[i]\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "       \n",
    "        # within-groups analysis only has: SAME, REVERSE, DIFFERENT\n",
    "        diff_word_group = []\n",
    "        reverse_word_group = []\n",
    "        same_word_group = [] \n",
    "        \n",
    "        ## 01: Create WordPair Groups\n",
    "        same_word_group, reverse_word_group, diff_word_group = createWordGroups(wordpairs)\n",
    "        \n",
    "        #### plot meta information about which session and blocks we're analyzing\n",
    "        plotDescription(session, block)\n",
    "        \n",
    "        ## 02: extract sessionblockdata dictionary for all channels\n",
    "        block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "        channels = np.arange(1, num_chans+1, 1)\n",
    "        \n",
    "        same_word_distances = createSameWordSimilarities(block_data, channels)\n",
    "        diff_word_distances = createDiffWordSimilarities(block_data, channels)\n",
    "        \n",
    "        if debug_on:\n",
    "            print \"done...\"\n",
    "            print \"Example paired word feature matrix: \", diff_feature_mat2.shape\n",
    "            print same_word_distances.shape\n",
    "            print diff_word_distances.shape\n",
    "        \n",
    "        ##### 01: Plot Histogram of Cosine Similarities\n",
    "        plotHistogramDistances(same_word_distances, session, block, 'same word pairs')\n",
    "        plotHistogramDistances(diff_word_distances, session, block, 'diff word pairs')\n",
    "        \n",
    "        ##### RUN STATS COMPARISONS ON SAME VS. REVERSE, SAME VS. DIFF, \n",
    "        random_subset = np.random.choice(range(same_word_distances.shape[0]), size=len(same_word_distances)/2, replace=False)\n",
    "        random_subset2 = list(set(np.arange(0, len(same_word_distances))) - set(random_subset))\n",
    "        same_X = same_word_distances[random_subset]\n",
    "        same_Y = same_word_distances[random_subset2]\n",
    "        \n",
    "        # sub-sample the diff_word_distances if using ks_test\n",
    "#         stat, same_p_val = stats.ks_2samp(same_X, same_Y)\n",
    "#         stat, diff_p_val = stats.ks_2samp(same_word_distances, diff_word_distances)\n",
    "           \n",
    "        stat, same_p_val = stats.ttest_ind(same_X, same_Y)\n",
    "        stat, diff_p_val = stats.ttest_ind(same_word_distances, diff_word_distances)\n",
    "        \n",
    "        print \"On block: \", block, \" using t-test\"\n",
    "        print \"Same avg +/- std: %0.3f\" %np.mean(same_word_distances), ' +/- %0.3f' %stats.sem(same_word_distances)\n",
    "        print \"Different avg +/- std: %0.3f\" %np.mean(diff_word_distances), ' +/- %0.3f' %stats.sem(diff_word_distances)\n",
    "        print \"Same vs. Same comparison: %0.3f\" %same_p_val\n",
    "        print \"Same vs. Different Comparison: %0.3f\" %diff_p_val, \"\\n\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Add PCA\n",
    "With this cosine similarity comparison of feature vectors from each channel comparison between word pairing events, the feature vector is of the order or ~400-600 elements long.\n",
    "\n",
    "Using PCA, I can reduce the dimensionality along the directions of greatest variance, and using Scree plots, get the components that encompass up to 99%, 95% and 90% of the variance for testing.\n",
    "\n",
    "In order to do PCA, I have to do the comparisons differently. First, instead of computing all combinations of cosine similarities, I will first subtract the average of the within blocks word pair that is matched.\n",
    "\n",
    "same pair comparison: AB - average(AB) \n",
    "diff pair comparison: AB - average(CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plotScree(sing_vals, eig_vals_ratio):\n",
    "    # plot scree plot\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    plt.plot(sing_vals, eig_vals_ratio, 'ro-', linewidth=2)\n",
    "    plt.title('Scree Plot For Frequency Vector Features For All Channels')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.axhline(0.9)\n",
    "    leg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3, \n",
    "         shadow=False, markerscale=0.2)\n",
    "\n",
    "def createSameWordsFeatureMat(block_data, channels):\n",
    "    same_word_distances = np.array(())\n",
    "    \n",
    "    ################# 02a: Same Words Cosine Distnace #################\n",
    "    # extract channel data for same word group\n",
    "    same_word_dict = {}\n",
    "    for same_words in same_word_group:\n",
    "        same_feature_mat = np.array(()) # initialize matrix to store feature vectors for each event\n",
    "\n",
    "        for chan in channels: \n",
    "            ## 01: extract data to process - average across time \n",
    "            same_word_data = []\n",
    "            same_word_key = same_words[0]\n",
    "            probeOnTime = block_data['data'][same_word_key][str(chan)]['timeZero']\n",
    "            vocalizationTime = block_data['data'][same_word_key][str(chan)]['timeVocalization']\n",
    "            powerMat = block_data['data'][same_word_key][str(chan)]['powerMat']\n",
    "\n",
    "            ## 02: average across time and append frequency feature vector\n",
    "            for i in range(0, len(vocalizationTime)):\n",
    "                # either go from timezero -> vocalization, or some other timewindow\n",
    "                feature_vect = np.mean(powerMat[i,freq_bands,vocalizationTime[i]-num_time_windows:vocalizationTime[i]-1],axis=1)\n",
    "                same_word_data.append(np.ndarray.flatten(feature_vect))\n",
    "\n",
    "            ## 03: append freq. feature vector per channel to each event\n",
    "            if same_feature_mat.size == 0:\n",
    "                same_feature_mat = np.array(same_word_data)\n",
    "            else:\n",
    "                same_feature_mat = np.append(same_feature_mat, np.array(same_word_data), axis=1)\n",
    "\n",
    "        ## 04: Store the events X features matrix\n",
    "        same_word_dict[same_word_key] = same_feature_mat\n",
    "\n",
    "    return same_word_dict\n",
    "\n",
    "def createDiffWordsFeatureMat(block_data, channels):\n",
    "    diff_word_distances = np.array(())\n",
    "   \n",
    "    ########################## 02b: DIFFERENT WORD PAIRS ########################################\n",
    "    diff_word_dict = {}\n",
    "    for diff_words in diff_word_group:\n",
    "        diff_feature_mat1 = np.array(()) # initialize matrix to store feature vectors for each event\n",
    "        diff_feature_mat2 = np.array(())\n",
    "\n",
    "        # keys for different words\n",
    "        diff_word1 = diff_words[0]\n",
    "        diff_word2 = diff_words[1] \n",
    "\n",
    "        for chan in channels:    # loop through every channel\n",
    "            # word keys and buffer lists\n",
    "            diff_word_databuffer1 = []\n",
    "            diff_word_databuffer2 = []\n",
    "\n",
    "            ## 01: extract channel data from blockdata dictionary\n",
    "            probeOnTime = block_data['data'][diff_word1][str(chan)]['timeZero']\n",
    "            vocalizationTime = block_data['data'][diff_word1][str(chan)]['timeVocalization']\n",
    "            powerMat = block_data['data'][diff_word1][str(chan)]['powerMat']\n",
    "            ## 02: average across time and append frequency feature vector\n",
    "            for i in range(0, len(vocalizationTime)):\n",
    "                feature_vect = np.mean(powerMat[i,freq_bands,vocalizationTime[i]-num_time_windows:vocalizationTime[i]-1],axis=1)\n",
    "                diff_word_databuffer1.append(np.ndarray.flatten(feature_vect))\n",
    "            ## 03:append freq. feature vector per channel to each event\n",
    "            if diff_feature_mat1.size == 0:\n",
    "                diff_feature_mat1 = np.array(diff_word_databuffer1)\n",
    "            else:\n",
    "                diff_feature_mat1 = np.append(diff_feature_mat1, np.array(diff_word_databuffer1), axis=1)\n",
    "\n",
    "            probeOnTime = block_data['data'][diff_word2][str(chan)]['timeZero']\n",
    "            vocalizationTime = block_data['data'][diff_word2][str(chan)]['timeVocalization']\n",
    "            powerMat = block_data['data'][diff_word2][str(chan)]['powerMat']\n",
    "            ## 02: average across time and append frequency feature vector\n",
    "            for i in range(0, len(vocalizationTime)):\n",
    "                feature_vect = np.mean(powerMat[i,freq_bands,vocalizationTime[i]-num_time_windows:vocalizationTime[i]-1],axis=1)\n",
    "                diff_word_databuffer2.append(np.ndarray.flatten(feature_vect))\n",
    "            ## 03:append freq. feature vector per channel to each event\n",
    "            if diff_feature_mat2.size == 0:\n",
    "                diff_feature_mat2 = np.array(diff_word_databuffer2)\n",
    "            else:\n",
    "                diff_feature_mat2 = np.append(diff_feature_mat2, np.array(diff_word_databuffer2), axis=1)\n",
    "        \n",
    "        pairWordKey = 'vs'.join(diff_words)\n",
    "        diff_word_dict[pairWordKey] = []\n",
    "        diff_word_dict[pairWordKey].append(diff_feature_mat1)\n",
    "        diff_word_dict[pairWordKey].append(diff_feature_mat2)\n",
    "        ## 04: pairwise comparison\n",
    "#         diff_word_dict['vs'.join(diff_words)] = computePairDistances(diff_feature_mat1, diff_feature_mat2)\n",
    "\n",
    "    return diff_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIH034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH034'\n",
    "filedir = '../../condensed_data_' + subj + '/sessions_05122016/'\n",
    "sessions = os.listdir(filedir)\n",
    "sessions = sessions[2:]\n",
    "num_chans = 96\n",
    "debug_on = 1\n",
    "\n",
    "## PCA OPTIONS:\n",
    "num_comp = 20\n",
    "sklearn_pca = PCA(n_components=num_comp)\n",
    "\n",
    "# loop through each session\n",
    "for session in sessions:\n",
    "    print \"Analyzing session \", session\n",
    "    sessiondir = filedir + session\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "\n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 6):\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "       \n",
    "        # within-groups analysis only has: SAME, REVERSE, DIFFERENT\n",
    "        diff_word_group = []\n",
    "        reverse_word_group = []\n",
    "        same_word_group = [] \n",
    "        \n",
    "        ## 01: Create WordPair Groups\n",
    "        same_word_group, reverse_word_group, diff_word_group = createWordGroups(wordpairs)\n",
    "        \n",
    "        #### plot meta information about which session and blocks we're analyzing\n",
    "        plotDescription(session, block)\n",
    "        \n",
    "        ## 02: extract sessionblockdata dictionary for all channels\n",
    "        block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "        channels = np.arange(1, num_chans+1, 1)\n",
    "        \n",
    "        ## Create the feature matrices per word\n",
    "        same_word_dict = createSameWordsFeatureMat(block_data, channels)\n",
    "        diff_word_dict = createDiffWordsFeatureMat(block_data, channels)\n",
    "        \n",
    "        ## loop through and create an events X feature matrix for PCA\n",
    "        feature_mat = np.array(()) # the overall eventsXfeatures matrix\n",
    "        feature_classes = [] # list of indices at which a new word pair is concatenated\n",
    "        for key in same_word_dict.keys():\n",
    "            if feature_mat.size == 0:\n",
    "                feature_mat = same_word_dict[key] - np.mean(same_word_dict[key], axis=0)\n",
    "                feature_classes.append(same_word_dict[key].shape[0])\n",
    "            else:\n",
    "                feature_mat = np.append(feature_mat, same_word_dict[key] - np.mean(same_word_dict[key], axis=0), axis=0)\n",
    "                feature_classes.append(feature_classes[len(feature_classes)-1] + same_word_dict[key].shape[0])\n",
    "        for key in diff_word_dict.keys():\n",
    "            diff_word_features = np.append(diff_word_dict[key][0] - np.mean(diff_word_dict[key][0], axis=0), diff_word_dict[key][1] - np.mean(diff_word_dict[key][1], axis=0), axis=0)\n",
    "            feature_mat = np.append(feature_mat, diff_word_features, axis=0)\n",
    "            feature_classes.append(feature_classes[len(feature_classes)-1] + diff_word_dict[key][0].shape[0])\n",
    "            feature_classes.append(feature_classes[len(feature_classes)-1] + diff_word_dict[key][1].shape[0])\n",
    "        \n",
    "        ## PERFORM PCA \n",
    "        num_comp = 76\n",
    "        sklearn_pca = PCA(n_components=num_comp)\n",
    "        sklearn_transf = sklearn_pca.fit_transform(feature_mat)\n",
    "        sklearn_variance_explained = sklearn_pca.explained_variance_ratio_\n",
    "        \n",
    "        # get singular values and their explained variance\n",
    "        sing_vals = np.arange(num_comp)\n",
    "        eig_vals_ratio = [sum(sklearn_variance_explained[0:i]) for i in range(0,len(sklearn_variance_explained))]\n",
    "\n",
    "        ## Loop through and compute cosine similarities between same/diff pair presentation\n",
    "        same_word_distances = np.array(())\n",
    "        diff_word_distances = np.array(())\n",
    "        for iClass in range(0, 4): # first 4 are same word pairs\n",
    "            if same_word_distances.size == 0:\n",
    "                same_word_distances = computeWithinDistances(sklearn_transf[0:feature_classes[iClass], :])\n",
    "            else:\n",
    "                same_word_distances = np.append(same_word_distances, computeWithinDistances(sklearn_transf[feature_classes[iClass-1]:feature_classes[iClass],:]), axis=0)\n",
    "        # next 8 are diff word pairs\n",
    "        for iClass in range(4, 12, 2): # first 4 are same word pairs\n",
    "            firstWordPair = sklearn_transf[feature_classes[iClass-1]:feature_classes[iClass], :]\n",
    "            secondWordPair = sklearn_transf[feature_classes[iClass]:feature_classes[iClass+1], :]\n",
    "            if diff_word_distances.size == 0:\n",
    "                diff_word_distances = computePairDistances(firstWordPair, secondWordPair)\n",
    "            else:\n",
    "                diff_word_distances = np.append(diff_word_distances, computePairDistances(firstWordPair, secondWordPair), axis=0)\n",
    "               \n",
    "        if debug_on:\n",
    "            print \"done...\"\n",
    "            print \"Feature matrix: \", feature_mat.shape\n",
    "            print \"Number of components used: \", num_comp\n",
    "            print same_word_distances.shape\n",
    "            print diff_word_distances.shape\n",
    "            print sklearn_transf.shape\n",
    "            plotScree(sing_vals, eig_vals_ratio)\n",
    "            debug_on = False\n",
    "        \n",
    "        ##### 01: Plot Histogram of Cosine Similarities\n",
    "        plotHistogramDistances(same_word_distances, session, block, 'same word pairs')\n",
    "        plotHistogramDistances(diff_word_distances, session, block, 'diff word pairs')\n",
    "        \n",
    "        ##### RUN STATS COMPARISONS ON SAME VS. REVERSE, SAME VS. DIFF, \n",
    "        random_subset = np.random.choice(range(same_word_distances.shape[0]), size=len(same_word_distances)/2, replace=False)\n",
    "        random_subset2 = list(set(np.arange(0, len(same_word_distances))) - set(random_subset))\n",
    "        same_X = same_word_distances[random_subset]\n",
    "        same_Y = same_word_distances[random_subset2]\n",
    "        \n",
    "        # sub-sample the diff_word_distances if using ks_test\n",
    "#         stat, same_p_val = stats.ks_2samp(same_X, same_Y)\n",
    "#         stat, diff_p_val = stats.ks_2samp(same_word_distances, diff_word_distances)\n",
    "           \n",
    "        stat, same_p_val = stats.ttest_ind(same_X, same_Y)\n",
    "        stat, diff_p_val = stats.ttest_ind(same_word_distances, diff_word_distances)\n",
    "        \n",
    "        print \"On block: \", block, \" using t-test\"\n",
    "        print \"Same avg +/- std: %0.6f\" %np.mean(same_word_distances), ' +/- %0.3f' %stats.sem(same_word_distances)\n",
    "        print \"Different avg +/- std: %0.6f\" %np.mean(diff_word_distances), ' +/- %0.3f' %stats.sem(diff_word_distances)\n",
    "        print \"Same vs. Same comparison: %0.6f\" %same_p_val\n",
    "        print \"Same vs. Different Comparison: %0.6f\" %diff_p_val, \"\\n\"\n",
    "#         break # for 1 block\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIH039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Get list of files (.mat) we want to work with ########\n",
    "subj = 'NIH039'\n",
    "filedir = '../../condensed_data_' + subj + '/sessions_05122016/'\n",
    "sessions = os.listdir(filedir)\n",
    "# sessions = sessions[2:]\n",
    "num_chans = 72\n",
    "debug_on = 1\n",
    "\n",
    "## PCA OPTIONS:\n",
    "num_comp = 20\n",
    "sklearn_pca = PCA(n_components=num_comp)\n",
    "\n",
    "# loop through each session\n",
    "for session in sessions:\n",
    "    print \"Analyzing session \", session\n",
    "    sessiondir = filedir + session\n",
    "    \n",
    "    # get all blocks for this session\n",
    "    blocks = os.listdir(sessiondir)\n",
    "\n",
    "    # loop through each block one at a time, analyze\n",
    "    for i in range(0, 6):\n",
    "        block = blocks[i]\n",
    "        block_dir = sessiondir + '/' + block\n",
    "        \n",
    "        # in each block, get list of word pairs from first and second block\n",
    "        wordpairs = os.listdir(block_dir)\n",
    "       \n",
    "        # within-groups analysis only has: SAME, REVERSE, DIFFERENT\n",
    "        diff_word_group = []\n",
    "        reverse_word_group = []\n",
    "        same_word_group = [] \n",
    "        \n",
    "        ## 01: Create WordPair Groups\n",
    "        same_word_group, reverse_word_group, diff_word_group = createWordGroups(wordpairs)\n",
    "        \n",
    "        #### plot meta information about which session and blocks we're analyzing\n",
    "        plotDescription(session, block)\n",
    "        \n",
    "        ## 02: extract sessionblockdata dictionary for all channels\n",
    "        block_data = extractSubjSessionBlockData(subj, session, block)\n",
    "        channels = np.arange(1, num_chans+1, 1)\n",
    "        \n",
    "        ## Create the feature matrices per word\n",
    "        same_word_dict = createSameWordsFeatureMat(block_data, channels)\n",
    "        diff_word_dict = createDiffWordsFeatureMat(block_data, channels)\n",
    "        \n",
    "        ## loop through and create an events X feature matrix for PCA\n",
    "        feature_mat = np.array(()) # the overall eventsXfeatures matrix\n",
    "        feature_classes = [] # list of indices at which a new word pair is concatenated\n",
    "        for key in same_word_dict.keys():\n",
    "            if feature_mat.size == 0:\n",
    "                feature_mat = same_word_dict[key] - np.mean(same_word_dict[key], axis=0)\n",
    "                feature_classes.append(same_word_dict[key].shape[0])\n",
    "            else:\n",
    "                feature_mat = np.append(feature_mat, same_word_dict[key] - np.mean(same_word_dict[key], axis=0), axis=0)\n",
    "                feature_classes.append(feature_classes[len(feature_classes)-1] + same_word_dict[key].shape[0])\n",
    "        for key in diff_word_dict.keys():\n",
    "            diff_word_features = np.append(diff_word_dict[key][0] - np.mean(diff_word_dict[key][0], axis=0), diff_word_dict[key][1] - np.mean(diff_word_dict[key][1], axis=0), axis=0)\n",
    "            feature_mat = np.append(feature_mat, diff_word_features, axis=0)\n",
    "            feature_classes.append(feature_classes[len(feature_classes)-1] + diff_word_dict[key][0].shape[0])\n",
    "            feature_classes.append(feature_classes[len(feature_classes)-1] + diff_word_dict[key][1].shape[0])\n",
    "        \n",
    "        ## PERFORM PCA \n",
    "        num_comp = 60\n",
    "        sklearn_pca = PCA(n_components=num_comp)\n",
    "        sklearn_transf = sklearn_pca.fit_transform(feature_mat)\n",
    "        sklearn_variance_explained = sklearn_pca.explained_variance_ratio_\n",
    "        \n",
    "        # get singular values and their explained variance\n",
    "        sing_vals = np.arange(num_comp)\n",
    "        eig_vals_ratio = [sum(sklearn_variance_explained[0:i]) for i in range(0,len(sklearn_variance_explained))]\n",
    "\n",
    "        ## Loop through and compute cosine similarities between same/diff pair presentation\n",
    "        same_word_distances = np.array(())\n",
    "        diff_word_distances = np.array(())\n",
    "        for iClass in range(0, 4): # first 4 are same word pairs\n",
    "            if same_word_distances.size == 0:\n",
    "                same_word_distances = computeWithinDistances(sklearn_transf[0:feature_classes[iClass], :])\n",
    "            else:\n",
    "                same_word_distances = np.append(same_word_distances, computeWithinDistances(sklearn_transf[feature_classes[iClass-1]:feature_classes[iClass],:]), axis=0)\n",
    "        # next 8 are diff word pairs\n",
    "        for iClass in range(4, 12, 2): # first 4 are same word pairs\n",
    "            firstWordPair = sklearn_transf[feature_classes[iClass-1]:feature_classes[iClass], :]\n",
    "            secondWordPair = sklearn_transf[feature_classes[iClass]:feature_classes[iClass+1], :]\n",
    "            if diff_word_distances.size == 0:\n",
    "                diff_word_distances = computePairDistances(firstWordPair, secondWordPair)\n",
    "            else:\n",
    "                diff_word_distances = np.append(diff_word_distances, computePairDistances(firstWordPair, secondWordPair), axis=0)\n",
    "               \n",
    "        if debug_on:\n",
    "            print \"done...\"\n",
    "            print \"Feature matrix: \", feature_mat.shape\n",
    "            print \"Number of components used: \", num_comp\n",
    "            print same_word_distances.shape\n",
    "            print diff_word_distances.shape\n",
    "            print sklearn_transf.shape\n",
    "            plotScree(sing_vals, eig_vals_ratio)\n",
    "            debug_on = False\n",
    "        \n",
    "        ##### 01: Plot Histogram of Cosine Similarities\n",
    "        plotHistogramDistances(same_word_distances, session, block, 'same word pairs')\n",
    "        plotHistogramDistances(diff_word_distances, session, block, 'diff word pairs')\n",
    "        \n",
    "        ##### RUN STATS COMPARISONS ON SAME VS. REVERSE, SAME VS. DIFF, \n",
    "        random_subset = np.random.choice(range(same_word_distances.shape[0]), size=len(same_word_distances)/2, replace=False)\n",
    "        random_subset2 = list(set(np.arange(0, len(same_word_distances))) - set(random_subset))\n",
    "        same_X = same_word_distances[random_subset]\n",
    "        same_Y = same_word_distances[random_subset2]\n",
    "        \n",
    "        # sub-sample the diff_word_distances if using ks_test\n",
    "#         stat, same_p_val = stats.ks_2samp(same_X, same_Y)\n",
    "#         stat, diff_p_val = stats.ks_2samp(same_word_distances, diff_word_distances)\n",
    "           \n",
    "        stat, same_p_val = stats.ttest_ind(same_X, same_Y)\n",
    "        stat, diff_p_val = stats.ttest_ind(same_word_distances, diff_word_distances)\n",
    "        \n",
    "        print \"On block: \", block, \" using t-test\"\n",
    "        print \"Same avg +/- std: %0.6f\" %np.mean(same_word_distances), ' +/- %0.3f' %stats.sem(same_word_distances)\n",
    "        print \"Different avg +/- std: %0.6f\" %np.mean(diff_word_distances), ' +/- %0.3f' %stats.sem(diff_word_distances)\n",
    "        print \"Same vs. Same comparison: %0.6f\" %same_p_val\n",
    "        print \"Same vs. Different Comparison: %0.6f\" %diff_p_val, \"\\n\"\n",
    "#         break # for 1 block\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion:\n",
    "\n",
    "After principle component analysis, it seems all the same vs. same comparisons are significantly different from same vs. different. The main visual difference is in the spread of the cosine similarities.\n",
    "\n",
    "However, can we really say this is true?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
